{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from __future__ import print_function\n",
    "from __future__ import division\n",
    "\n",
    "import pickle\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import scipy.sparse as sp\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('data/dti_store/graph_2.pkl') as f:\n",
    "    graph = pickle.load(f)\n",
    "num_u, num_v, u_nodes, v_nodes, ratings, u_feat, v_feat = graph"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "u_feat = u_feat.toarray()\n",
    "v_feat = v_feat.toarray()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(0, 1861, 0, 1553)"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.min(u_nodes), np.max(u_nodes), np.min(v_nodes), np.max(v_nodes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = []\n",
    "for i in range(len(u_nodes)):\n",
    "    x = np.concatenate([u_feat[u_nodes[i]], v_feat[v_nodes[i]]], axis=0)\n",
    "    X.append(x)\n",
    "X = np.vstack(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(104809, 1757)"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "y = ratings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(104809,)"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from time import time\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.model_selection import train_test_split, GridSearchCV"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(X, y, stratify=y, test_size=0.2, shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((83847, 1757), (20962, 1757), (83847,), (20962,))"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train.shape, X_test.shape, y_train.shape, y_test.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Time taken = 169.81208396 s\n"
     ]
    }
   ],
   "source": [
    "t0 = time()\n",
    "dt = DecisionTreeClassifier()\n",
    "dt.fit(X_train, y_train)\n",
    "print (\"Time taken =\", time() - t0, \"s\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "pred = dt.predict_proba(X_test)[:, 1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import roc_auc_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.7833585758835759"
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "roc_auc_score(y_test, pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[1., 0.],\n",
       "       [1., 0.],\n",
       "       [1., 0.],\n",
       "       ...,\n",
       "       [1., 0.],\n",
       "       [1., 0.],\n",
       "       [1., 0.]])"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Time taken = 11.9432201385 s\n"
     ]
    }
   ],
   "source": [
    "t1 = time()\n",
    "rf = RandomForestClassifier()\n",
    "rf.fit(X_train, y_train)\n",
    "print (\"Time taken =\", time() - t1, \"s\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.8147317567567568"
      ]
     },
     "execution_count": 53,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pred = rf.predict_proba(X_test)[:, 1]\n",
    "roc_auc_score(y_test, pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tqdm import tqdm\n",
    "\n",
    "class EnsemDT():\n",
    "    \n",
    "    def __init__(self, subset_features=False,\n",
    "                    num_base_learners=100,\n",
    "                    np_ratio=5, feat_dim=100,\n",
    "                    max_depth=None):\n",
    "        \"\"\" EnsemDT class from paper by Ezzat et al.(2017).\n",
    "            It is a bagging ensemble of Decision Trees with\n",
    "            a focus on class imbalance.\n",
    "            \n",
    "            :params subset_features: Set true to use feature\n",
    "                subsetting.\n",
    "            :params num_base_learners: Number of base learners\n",
    "                (decision trees) to use.\n",
    "            :params np_ratio: positive to negative samples\n",
    "                ratio.\n",
    "            :params feat_dim: Number of features for \n",
    "                subsetting.\n",
    "            :params max_depth: Maximum depth of each\n",
    "                individual learner. \"\"\"\n",
    "        \n",
    "        self.num_base_learners = num_base_learners\n",
    "        self.np_ratio = np_ratio\n",
    "        self.feat_dim = feat_dim\n",
    "        self.subset_features = subset_features\n",
    "        self.max_depth = max_depth\n",
    "        \n",
    "        clfs = list()\n",
    "        for i in range(self.num_base_learners):\n",
    "            clfs.append(DecisionTreeClassifier(max_depth=self.max_depth))\n",
    "        self.clfs = clfs\n",
    "        \n",
    "        self.clf_fit = False\n",
    "        \n",
    "    def fit(self, X_pos, X_neg):\n",
    "        \"\"\" Fit EnsemDT on the dataset.\n",
    "            \n",
    "            X_pos and X_neg must be DataFrames\n",
    "            with label as a column 'y' in each of\n",
    "            them.\n",
    "            \n",
    "            :params X_pos: Dataset with +ve samples.\n",
    "            :params X_neg: Dataset with -ve samples.\n",
    "        \"\"\"\n",
    "        \n",
    "        self.X_pos = X_pos\n",
    "        self.X_neg = X_neg\n",
    "        \n",
    "        self.y_pos = X_pos['y']\n",
    "        self.y_neg = X_neg['y']\n",
    "        \n",
    "        self.num_pos = X_pos.shape[0]\n",
    "        self.num_neg = X_neg.shape[0]\n",
    "         \n",
    "        for i in tqdm(range(self.num_base_learners),\n",
    "                     desc='Training learners...',\n",
    "                     unit='learners'):\n",
    "            \n",
    "            # Random sampling\n",
    "            X_neg_i = self.X_neg.sample(self.num_pos * self.np_ratio)\n",
    "            X_pos_i = self.X_pos\n",
    "            \n",
    "            # Merge dataset\n",
    "            X_i = pd.concat([X_neg_i, X_pos_i])\n",
    "            y_i = X_i['y']\n",
    "            X_i.drop(['y'], axis=1, inplace=True)\n",
    "            \n",
    "            # Feature subsetting\n",
    "            if self.subset_features:\n",
    "                X_i = X_i.sample(self.feat_dim, \n",
    "                                 axis=1)\n",
    "            \n",
    "            self.clfs[i].fit(X_i, y_i)\n",
    "            \n",
    "        self.clf_fit = True\n",
    "            \n",
    "    def get_scores(self, X_val):\n",
    "        \"\"\" Returns scores of classes. The\n",
    "            score is directly related to the class\n",
    "            predicted. \n",
    "            \n",
    "            :params X_val: Validation set (or test). \"\"\"\n",
    "        \n",
    "        if not self.clf_fit:\n",
    "            raise RuntimeError('Call clf.fit before clf.predict.')\n",
    "        \n",
    "        # Create predictions from learners\n",
    "        preds = list()\n",
    "        for i in range(self.num_base_learners):\n",
    "            pred = self.clfs[i].predict(X_val)\n",
    "            preds.append(pred)\n",
    "            \n",
    "        # Average results\n",
    "        preds = np.vstack(preds)\n",
    "        preds = preds.T\n",
    "        \n",
    "        scores = list()\n",
    "        for pred in preds:\n",
    "            scores.append(float(sum(pred))/float(preds.shape[1]))\n",
    "            \n",
    "        return scores\n",
    "    \n",
    "    def predict(self, X_val):\n",
    "        \"\"\" Predict labels for the given validation\n",
    "            set (0 or 1). Calls the get_scores function\n",
    "            for prediction. \n",
    "            \n",
    "            :params X_val: Validation set (or test). \"\"\"\n",
    "        \n",
    "        # Get scores\n",
    "        preds = list()\n",
    "        scores = self.get_scores(X_val)\n",
    "\n",
    "        # Round to predictions\n",
    "        for score in scores:\n",
    "            preds.append(round(score))\n",
    "    \n",
    "        # Read as numpy array\n",
    "        preds = np.array(preds).astype('int32')\n",
    "        \n",
    "        return preds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(array([[1., 0., 0., ..., 0., 0., 0.],\n",
       "        [1., 1., 0., ..., 0., 0., 0.],\n",
       "        [1., 1., 1., ..., 0., 0., 0.],\n",
       "        ...,\n",
       "        [1., 1., 1., ..., 0., 0., 0.],\n",
       "        [1., 1., 1., ..., 0., 0., 0.],\n",
       "        [1., 1., 0., ..., 0., 0., 0.]], dtype=float32),\n",
       " array([[1., 1., 1., ..., 0., 0., 0.],\n",
       "        [1., 1., 1., ..., 0., 0., 0.],\n",
       "        [1., 1., 0., ..., 0., 0., 0.],\n",
       "        ...,\n",
       "        [1., 1., 1., ..., 0., 0., 0.],\n",
       "        [1., 1., 0., ..., 0., 0., 0.],\n",
       "        [1., 1., 1., ..., 0., 0., 0.]], dtype=float32),\n",
       " array([0, 0, 0, ..., 0, 0, 0]),\n",
       " array([0, 0, 0, ..., 0, 0, 0]))"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train, X_test, y_train, y_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.15rc1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
