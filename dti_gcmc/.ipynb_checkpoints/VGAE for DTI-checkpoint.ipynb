{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [],
   "source": [
    "from __future__ import division\n",
    "from __future__ import print_function\n",
    "\n",
    "import time\n",
    "import pickle\n",
    "\n",
    "import numpy as np\n",
    "import scipy.sparse as sp\n",
    "\n",
    "import networkx as nx\n",
    "import tensorflow as tf\n",
    "\n",
    "from collections import namedtuple\n",
    "from sklearn.metrics import roc_auc_score\n",
    "from sklearn.metrics import average_precision_score\n",
    "\n",
    "# from vgae_utils import load_data, glorot, weight_variable_glorot, dropout_sparse, \\\n",
    "#     sparse_to_tuple, preprocess_graph, construct_feed_dict, mask_test_edges, get_roc_score\n",
    "# from vgae_layers import GraphConvolutionSparse, GraphConvolution, Dense, InnerProductDecoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [],
   "source": [
    "def secs_to_mins(secs):\n",
    "    mins = int(secs / 60)\n",
    "    secs = int(secs % 60)\n",
    "    \n",
    "    if mins != 0:\n",
    "        time_in_mins = str(mins) + \"m \" + str(secs) + \"s\"\n",
    "    else:\n",
    "        time_in_mins = str(secs) + \"s\"\n",
    "        \n",
    "    return time_in_mins\n",
    "\n",
    "def load_data():\n",
    "    g = nx.read_edgelist('yeast.edgelist')\n",
    "    adj = nx.adjacency_matrix(g)\n",
    "    return adj\n",
    "\n",
    "def glorot(shape, name=None):\n",
    "    \"\"\"Glorot & Bengio (AISTATS 2010) init.\"\"\"\n",
    "    init_range = np.sqrt(6.0/(shape[0]+shape[1]))\n",
    "    initial = tf.random_uniform(shape, minval=-init_range, maxval=init_range, dtype=tf.float32)\n",
    "    return tf.Variable(initial, name=name)\n",
    "\n",
    "def weight_variable_glorot(input_dim, output_dim, name=\"\"):\n",
    "    init_range = np.sqrt(6.0 / (input_dim + output_dim))\n",
    "    initial = tf.random_uniform(\n",
    "        [input_dim, output_dim], minval=-init_range,\n",
    "        maxval=init_range, dtype=tf.float32)\n",
    "    return tf.Variable(initial, name=name)\n",
    "\n",
    "def dropout_sparse(x, keep_prob, num_nonzero_elems):\n",
    "    noise_shape = [num_nonzero_elems]\n",
    "    random_tensor = keep_prob\n",
    "    random_tensor += tf.random_uniform(noise_shape)\n",
    "    dropout_mask = tf.cast(tf.floor(random_tensor), dtype=tf.bool)\n",
    "    pre_out = tf.sparse_retain(x, dropout_mask)\n",
    "    return pre_out * (1. / keep_prob)\n",
    "\n",
    "def sparse_to_tuple(sparse_mx):\n",
    "    if not sp.isspmatrix_coo(sparse_mx):\n",
    "        sparse_mx = sparse_mx.tocoo()\n",
    "    coords = np.vstack((sparse_mx.row, sparse_mx.col)).transpose()\n",
    "    values = sparse_mx.data\n",
    "    shape = sparse_mx.shape\n",
    "    return coords, values, shape\n",
    "\n",
    "def preprocess_graph(adj):\n",
    "    adj = sp.coo_matrix(adj)\n",
    "    adj_ = adj + sp.eye(adj.shape[0])\n",
    "    rowsum = np.array(adj_.sum(1))\n",
    "    degree_mat_inv_sqrt = sp.diags(np.power(rowsum, -0.5).flatten())\n",
    "    adj_normalized = adj_.dot(degree_mat_inv_sqrt).transpose().dot(degree_mat_inv_sqrt).tocoo()\n",
    "    return sparse_to_tuple(adj_normalized)\n",
    "\n",
    "def construct_feed_dict(adj_normalized, adj, features, placeholders):\n",
    "    feed_dict = dict()\n",
    "    feed_dict.update({placeholders['features']: features})\n",
    "    feed_dict.update({placeholders['adj']: adj_normalized})\n",
    "    feed_dict.update({placeholders['adj_orig']: adj})\n",
    "    return feed_dict\n",
    "\n",
    "def mask_test_edges(adj):\n",
    "    # Function to build test set with 2% positive links\n",
    "    # Remove diagonal elements\n",
    "    adj = adj - sp.dia_matrix((adj.diagonal()[np.newaxis, :], [0]), shape=adj.shape)\n",
    "    adj.eliminate_zeros()\n",
    "\n",
    "    adj_triu = sp.triu(adj)\n",
    "    adj_tuple = sparse_to_tuple(adj_triu)\n",
    "    edges = adj_tuple[0]\n",
    "    edges_all = sparse_to_tuple(adj)[0]\n",
    "    num_test = int(np.floor(edges.shape[0] / 50.))\n",
    "    num_val = int(np.floor(edges.shape[0] / 50.))\n",
    "\n",
    "    all_edge_idx = range(edges.shape[0])\n",
    "    np.random.shuffle(all_edge_idx)\n",
    "    val_edge_idx = all_edge_idx[:num_val]\n",
    "    test_edge_idx = all_edge_idx[num_val:(num_val + num_test)]\n",
    "    test_edges = edges[test_edge_idx]\n",
    "    val_edges = edges[val_edge_idx]\n",
    "    train_edges = np.delete(edges, np.hstack([test_edge_idx, val_edge_idx]), axis=0)\n",
    "\n",
    "    def ismember(a, b):\n",
    "        rows_close = np.all((a - b[:, None]) == 0, axis=-1)\n",
    "        return np.any(rows_close)\n",
    "\n",
    "    test_edges_false = []\n",
    "    while len(test_edges_false) < len(test_edges):\n",
    "        n_rnd = len(test_edges) - len(test_edges_false)\n",
    "        rnd = np.random.randint(0, adj.shape[0], size=2 * n_rnd)\n",
    "        idxs_i = rnd[:n_rnd]                                        \n",
    "        idxs_j = rnd[n_rnd:]\n",
    "        for i in range(n_rnd):\n",
    "            idx_i = idxs_i[i]\n",
    "            idx_j = idxs_j[i]\n",
    "            if idx_i == idx_j:\n",
    "                continue\n",
    "            if ismember([idx_i, idx_j], edges_all):\n",
    "                continue\n",
    "            if test_edges_false:\n",
    "                if ismember([idx_j, idx_i], np.array(test_edges_false)):\n",
    "                    continue\n",
    "                if ismember([idx_i, idx_j], np.array(test_edges_false)):\n",
    "                    continue\n",
    "            test_edges_false.append([idx_i, idx_j])\n",
    "\n",
    "    val_edges_false = []\n",
    "    while len(val_edges_false) < len(val_edges):\n",
    "        n_rnd = len(val_edges) - len(val_edges_false)\n",
    "        rnd = np.random.randint(0, adj.shape[0], size=2 * n_rnd)\n",
    "        idxs_i = rnd[:n_rnd]                                        \n",
    "        idxs_j = rnd[n_rnd:]\n",
    "        for i in range(n_rnd):\n",
    "            idx_i = idxs_i[i]\n",
    "            idx_j = idxs_j[i]\n",
    "            if idx_i == idx_j:\n",
    "                continue\n",
    "            if ismember([idx_i, idx_j], train_edges):\n",
    "                continue\n",
    "            if ismember([idx_j, idx_i], train_edges):\n",
    "                continue\n",
    "            if ismember([idx_i, idx_j], val_edges):\n",
    "                continue\n",
    "            if ismember([idx_j, idx_i], val_edges):\n",
    "                continue\n",
    "            if val_edges_false:\n",
    "                if ismember([idx_j, idx_i], np.array(val_edges_false)):\n",
    "                    continue\n",
    "                if ismember([idx_i, idx_j], np.array(val_edges_false)):\n",
    "                    continue\n",
    "            val_edges_false.append([idx_i, idx_j])\n",
    "\n",
    "    # Re-build adj matrix\n",
    "    data = np.ones(train_edges.shape[0])\n",
    "    adj_train = sp.csr_matrix((data, (train_edges[:, 0], train_edges[:, 1])), shape=adj.shape)\n",
    "    adj_train = adj_train + adj_train.T\n",
    "\n",
    "    return adj_train, train_edges, val_edges, val_edges_false, test_edges, test_edges_false\n",
    "\n",
    "\n",
    "def get_roc_score(edges_pos, edges_neg):\n",
    "    feed_dict.update({placeholders['dropout']: 0})\n",
    "    emb = sess.run(model.embeddings, feed_dict=feed_dict)\n",
    "\n",
    "    def sigmoid(x):\n",
    "        return 1 / (1 + np.exp(-x))\n",
    "\n",
    "    # Predict on test set of edges\n",
    "    adj_rec = np.dot(emb, emb.T)\n",
    "    preds = []\n",
    "    pos = []\n",
    "    for e in edges_pos:\n",
    "        preds.append(sigmoid(adj_rec[e[0], e[1]]))\n",
    "        pos.append(adj_orig[e[0], e[1]])\n",
    "\n",
    "    preds_neg = []\n",
    "    neg = []\n",
    "    for e in edges_neg:\n",
    "        preds_neg.append(sigmoid(adj_rec[e[0], e[1]]))\n",
    "        neg.append(adj_orig[e[0], e[1]])\n",
    "\n",
    "    preds_all = np.hstack([preds, preds_neg])\n",
    "    labels_all = np.hstack([np.ones(len(preds)), np.zeros(len(preds))])\n",
    "    roc_score = roc_auc_score(labels_all, preds_all)\n",
    "    ap_score = average_precision_score(labels_all, preds_all)\n",
    "\n",
    "    return roc_score, ap_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Layer(object):\n",
    "    \"\"\"Base layer class. Defines basic API for all layer objects.\n",
    "    Implementation inspired by keras (http://keras.io).\n",
    "\n",
    "    # Properties\n",
    "        name: String, defines the variable scope of the layer.\n",
    "        logging: Boolean, switches Tensorflow histogram logging on/off\n",
    "\n",
    "    # Methods\n",
    "        _call(inputs): Defines computation graph of layer\n",
    "            (i.e. takes input, returns output)\n",
    "        __call__(inputs): Wrapper for _call()\n",
    "        _log_vars(): Log all variables\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, **kwargs):\n",
    "        allowed_kwargs = {'name', 'logging'}\n",
    "        for kwarg in kwargs.keys():\n",
    "            assert kwarg in allowed_kwargs, 'Invalid keyword argument: ' + kwarg\n",
    "        name = kwargs.get('name')\n",
    "        if not name:\n",
    "            layer = self.__class__.__name__.lower()\n",
    "            name = layer + '_' + str(get_layer_uid(layer))\n",
    "        self.name = name\n",
    "        self.vars = {}\n",
    "        logging = kwargs.get('logging', False)\n",
    "        self.logging = logging\n",
    "        self.sparse_inputs = False\n",
    "\n",
    "    def _call(self, inputs):\n",
    "        return inputs\n",
    "\n",
    "    def __call__(self, inputs):\n",
    "        with tf.name_scope(self.name):\n",
    "            if self.logging and not self.sparse_inputs:\n",
    "                tf.summary.histogram(self.name + '/inputs', inputs)\n",
    "            outputs = self._call(inputs)\n",
    "            if self.logging:\n",
    "                tf.summary.histogram(self.name + '/outputs', outputs)\n",
    "            return outputs\n",
    "\n",
    "    def _log_vars(self):\n",
    "        for var in self.vars:\n",
    "            tf.summary.histogram(self.name + '/vars/' + var, self.vars[var])\n",
    "\n",
    "\n",
    "class GraphConvolution():\n",
    "    \"\"\"Basic graph convolution layer for undirected graph without edge labels.\"\"\"\n",
    "    def __init__(self, input_dim, output_dim, adj, name, dropout=0., act=tf.nn.relu):\n",
    "        self.name = name\n",
    "        self.vars = {}\n",
    "        self.issparse = False\n",
    "        with tf.variable_scope(self.name + '_vars'):\n",
    "            self.vars['weights'] = weight_variable_glorot(input_dim, output_dim, name='weights')\n",
    "        self.dropout = dropout\n",
    "        self.adj = adj\n",
    "        self.act = act\n",
    "\n",
    "    def __call__(self, inputs):\n",
    "        with tf.name_scope(self.name):        \n",
    "            x = inputs\n",
    "            x = tf.nn.dropout(x, 1-self.dropout)\n",
    "            x = tf.matmul(x, self.vars['weights'])\n",
    "            x = tf.sparse_tensor_dense_matmul(self.adj, x)\n",
    "            outputs = self.act(x)\n",
    "        return outputs\n",
    "\n",
    "\n",
    "class GraphConvolutionSparse():\n",
    "    \"\"\"Graph convolution layer for sparse inputs.\"\"\"\n",
    "    def __init__(self, input_dim, output_dim, adj, features_nonzero, name, dropout=0., act=tf.nn.relu):\n",
    "        self.name = name\n",
    "        self.vars = {}\n",
    "        self.issparse = False\n",
    "        with tf.variable_scope(self.name + '_vars'):\n",
    "            self.vars['weights'] = weight_variable_glorot(input_dim, output_dim, name='weights')\n",
    "        self.dropout = dropout\n",
    "        self.adj = adj\n",
    "        self.act = act\n",
    "        self.issparse = True\n",
    "        self.features_nonzero = features_nonzero\n",
    "\n",
    "    def __call__(self, inputs):\n",
    "        with tf.name_scope(self.name):\n",
    "            x = inputs\n",
    "            x = dropout_sparse(x, 1-self.dropout, self.features_nonzero)\n",
    "            x = tf.sparse_tensor_dense_matmul(x, self.vars['weights'])\n",
    "            x = tf.sparse_tensor_dense_matmul(self.adj, x)\n",
    "            outputs = self.act(x)\n",
    "        return outputs\n",
    "    \n",
    "def dot(x, y, sparse=False):\n",
    "    \"\"\"Wrapper for tf.matmul (sparse vs dense).\"\"\"\n",
    "    if sparse:\n",
    "        res = tf.sparse_tensor_dense_matmul(x, y)\n",
    "    else:\n",
    "        res = tf.matmul(x, y)\n",
    "    return res\n",
    "    \n",
    "class Dense(Layer):\n",
    "    \"\"\"Dense layer.\"\"\"\n",
    "    def __init__(self, input_dim, output_dim, placeholders, dropout=0., sparse_inputs=False,\n",
    "                 act=tf.nn.relu, bias=False, featureless=False, **kwargs):\n",
    "        super(Dense, self).__init__(**kwargs)\n",
    "\n",
    "        if dropout is not None:\n",
    "            self.dropout = placeholders['dropout']\n",
    "        else:\n",
    "            self.dropout = 0.\n",
    "\n",
    "        self.act = act\n",
    "        self.sparse_inputs = sparse_inputs\n",
    "        self.featureless = featureless\n",
    "        self.bias = bias\n",
    "\n",
    "        # helper variable for sparse dropout\n",
    "        self.num_features_nonzero = placeholders['num_features_nonzero']\n",
    "\n",
    "        with tf.variable_scope(self.name + '_vars'):\n",
    "            self.vars['weights'] = weight_variable_glorot(input_dim, output_dim,\n",
    "                                          name='weights')\n",
    "            if self.bias:\n",
    "                self.vars['bias'] = zeros([output_dim], name='bias')\n",
    "\n",
    "        if self.logging:\n",
    "            self._log_vars()\n",
    "\n",
    "    def _call(self, inputs):\n",
    "        x = inputs\n",
    "\n",
    "        # dropout\n",
    "        if self.sparse_inputs:\n",
    "            x = sparse_dropout(x, 1-self.dropout, self.num_features_nonzero)\n",
    "        else:\n",
    "            x = tf.nn.dropout(x, 1-self.dropout)\n",
    "\n",
    "        # transform\n",
    "        output = dot(x, self.vars['weights'], sparse=self.sparse_inputs)\n",
    "\n",
    "        # bias\n",
    "        if self.bias:\n",
    "            output += self.vars['bias']\n",
    "\n",
    "        return self.act(output)\n",
    "    \n",
    "class InnerProductDecoder():\n",
    "    \"\"\"Decoder model layer for link prediction.\"\"\"\n",
    "    def __init__(self, input_dim, name, dropout=0., act=tf.nn.sigmoid):\n",
    "        self.name = name\n",
    "        self.issparse = False\n",
    "        self.dropout = dropout\n",
    "        self.act = act\n",
    "\n",
    "    def __call__(self, inputs):\n",
    "        with tf.name_scope(self.name):\n",
    "            inputs = tf.nn.dropout(inputs, 1-self.dropout)\n",
    "            x = tf.transpose(inputs)\n",
    "            x = tf.matmul(inputs, x)\n",
    "            x = tf.reshape(x, [-1])\n",
    "            outputs = self.act(x)\n",
    "        return outputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set random seed\n",
    "seed = 123\n",
    "np.random.seed(seed)\n",
    "tf.set_random_seed(seed)\n",
    "\n",
    "learning_rate = 0.001\n",
    "hidden1 = 1024\n",
    "hidden2 = 512\n",
    "hidden3 = 256\n",
    "hidden4 = 128\n",
    "hidden5 = 256\n",
    "# hidden6 = 32\n",
    "# hidden7 = 64\n",
    "dropout = 0.1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [],
   "source": [
    "class GCNModel():\n",
    "    def __init__(self, placeholders, num_features, features_nonzero, name):\n",
    "        self.name = name\n",
    "        self.placeholders = placeholders\n",
    "        self.inputs = placeholders['features']\n",
    "        self.input_dim = num_features\n",
    "        self.features_nonzero = features_nonzero\n",
    "        self.adj = placeholders['adj']\n",
    "        self.dropout = placeholders['dropout']\n",
    "        with tf.variable_scope(self.name):\n",
    "            self.build()\n",
    "        \n",
    "    def build(self):\n",
    "        self.hidden1 = GraphConvolutionSparse(\n",
    "            name='gcn_sparse_layer',\n",
    "            input_dim=self.input_dim,\n",
    "            output_dim=hidden1,\n",
    "            adj=self.adj,\n",
    "            features_nonzero=self.features_nonzero,\n",
    "            act=tf.nn.relu,\n",
    "            dropout=self.dropout)(self.inputs)\n",
    "        \n",
    "        self.hidden2 = GraphConvolution(\n",
    "            name='gcn_hidden1',\n",
    "            input_dim=hidden1,\n",
    "            output_dim=hidden2,\n",
    "            adj=self.adj,\n",
    "            act=tf.nn.relu6,\n",
    "            dropout=self.dropout)(self.hidden1)\n",
    "        \n",
    "        self.hidden3 = Dense(\n",
    "            name='gcn_hidden2',\n",
    "            input_dim=hidden2,\n",
    "            output_dim=hidden3,\n",
    "            placeholders=self.placeholders,\n",
    "            act=tf.nn.relu6,\n",
    "            dropout=self.dropout)(self.hidden2)\n",
    "        \n",
    "        self.hidden4 = Dense(\n",
    "            name='gcn_hidden3',\n",
    "            input_dim=hidden3,\n",
    "            output_dim=hidden4,\n",
    "            placeholders=self.placeholders,\n",
    "            act=tf.nn.relu6,\n",
    "            dropout=self.dropout)(self.hidden3)\n",
    "\n",
    "#         self.hidden5 = Dense(\n",
    "#             name='dense',\n",
    "#             input_dim=hidden4,\n",
    "#             output_dim=hidden5,\n",
    "#             placeholders=self.placeholders,\n",
    "#             dropout=self.dropout)(self.hidden4)\n",
    "        \n",
    "#         self.hidden5 = GraphConvolution(\n",
    "#             name='gcn_hidden4',\n",
    "#             input_dim=hidden4,\n",
    "#             output_dim=hidden5,\n",
    "#             adj=self.adj,\n",
    "#             act=tf.nn.relu6,\n",
    "#             dropout=self.dropout)(self.hidden4)\n",
    "\n",
    "        self.embeddings = Dense(\n",
    "            name='gcn_dense_layer',\n",
    "            input_dim=hidden4,\n",
    "            output_dim=hidden5,\n",
    "            placeholders=self.placeholders,\n",
    "            act=lambda x: x,\n",
    "            dropout=self.dropout)(self.hidden4)\n",
    "\n",
    "#         self.hidden6 = GraphConvolution(\n",
    "#             name='gcn_hidden5',\n",
    "#             input_dim=hidden6,\n",
    "#             output_dim=hidden7,\n",
    "#             adj=self.adj,\n",
    "#             act=lambda x: x,\n",
    "#             dropout=self.dropout)(self.embeddings)\n",
    "        \n",
    "        self.reconstructions = InnerProductDecoder(\n",
    "            name='gcn_decoder',\n",
    "            input_dim=hidden5, \n",
    "            act=lambda x: x)(self.embeddings)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Optimizer():\n",
    "    def __init__(self, preds, labels, num_nodes, num_edges):\n",
    "        pos_weight = float(num_nodes**2) / num_edges\n",
    "        norm = num_nodes**2 / float((num_nodes**2 - num_edges) * 2)\n",
    "        \n",
    "        preds_sub = preds\n",
    "        labels_sub = labels\n",
    "        \n",
    "        self.cost = norm * tf.reduce_mean(\n",
    "            tf.nn.weighted_cross_entropy_with_logits(\n",
    "                logits=preds_sub, targets=labels_sub, pos_weight=pos_weight))\n",
    "        self.optimizer = tf.train.AdamOptimizer(learning_rate=learning_rate)  # Adam Optimizer\n",
    "\n",
    "        self.opt_op = self.optimizer.minimize(self.cost)\n",
    "        self.grads_vars = self.optimizer.compute_gradients(self.cost)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('data/dti_store/graph_2.pkl') as f:\n",
    "    graph = pickle.load(f)\n",
    "    \n",
    "u_nodes, v_nodes, ratings = graph[2], graph[3], graph[4]\n",
    "g = np.vstack([u_nodes, v_nodes, ratings])\n",
    "\n",
    "g = g.T\n",
    "g = g[g[:, 2].argsort()]\n",
    "g_0, g_1 = np.split(g, np.where(np.diff(g[:,2]))[0]+1)\n",
    "\n",
    "graph_pos = g_1\n",
    "graph_pos = graph_pos.T\n",
    "\n",
    "x, y, _ = graph_pos\n",
    "\n",
    "x = x.reshape((1, x.shape[0]))\n",
    "y = y.reshape((1, y.shape[0]))\n",
    "graph_pos = np.concatenate([x, y], axis=0)\n",
    "\n",
    "graph_pos = graph_pos.T\n",
    "\n",
    "for i in range(len(graph_pos)):\n",
    "    graph_pos[i][1] += 1861\n",
    "    \n",
    "adj = list()\n",
    "for s, d in graph_pos:\n",
    "    adj.append((s, d))\n",
    "    \n",
    "G = nx.Graph(adj)\n",
    "\n",
    "adj = nx.adj_matrix(G)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_nodes = adj.shape[0]\n",
    "num_edges = adj.sum()\n",
    "\n",
    "# # Featureless\n",
    "u_feat = np.array(graph[5].todense())\n",
    "v_feat = np.array(graph[6].todense())\n",
    "\n",
    "padded_v = tf.keras.preprocessing.sequence.pad_sequences(v_feat, maxlen=u_feat.shape[1], dtype='float32', padding='post', truncating='post')\n",
    "features = np.concatenate([u_feat, padded_v], axis=0)\n",
    "# features = tf.keras.preprocessing.sequence.pad_sequences(features, maxlen=u_feat.shape[0] + v_feat.shape[0], dtype='int64', padding='post', truncating='post')\n",
    "features = features[:-1,:-1]\n",
    "\n",
    "features = sp.csr_matrix(features)\n",
    "\n",
    "features = sparse_to_tuple(features)\n",
    "num_features = features[2][1]\n",
    "features_nonzero = features[1].shape[0]\n",
    "\n",
    "# Store original adjacency matrix (without diagonal entries) for later\n",
    "adj_orig = adj - sp.dia_matrix((adj.diagonal()[np.newaxis, :], [0]), shape=adj.shape)\n",
    "adj_orig.eliminate_zeros()\n",
    "\n",
    "adj_train, train_edges, val_edges, val_edges_false, test_edges, test_edges_false = mask_test_edges(adj)\n",
    "adj = adj_train\n",
    "\n",
    "adj_norm = preprocess_graph(adj)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define placeholders\n",
    "placeholders = {\n",
    "    'features': tf.sparse_placeholder(tf.float32),\n",
    "    'adj': tf.sparse_placeholder(tf.float32),\n",
    "    'adj_orig': tf.sparse_placeholder(tf.float32),\n",
    "    'dropout': tf.placeholder_with_default(0., shape=()),\n",
    "    'num_features_nonzero': tf.placeholder(tf.int32)\n",
    "}\n",
    "\n",
    "# Create model\n",
    "model = GCNModel(placeholders, num_features, features_nonzero, name='dti_gcn')\n",
    "\n",
    "# Create optimizer\n",
    "with tf.name_scope('optimizer'):\n",
    "    opt = Optimizer(\n",
    "        preds=model.reconstructions,\n",
    "        labels=tf.reshape(tf.sparse_tensor_to_dense(placeholders['adj_orig'], validate_indices=False), [-1]),\n",
    "        num_nodes=num_nodes,\n",
    "        num_edges=num_edges)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 0001 train_loss= 0.74646 val_roc= 0.68251 val_ap= 0.73671 test_roc= 0.65088 ap_test= 0.67954 time= 1.97340\n",
      "Epoch: 0002 train_loss= 0.73708 val_roc= 0.67372 val_ap= 0.73380 test_roc= 0.62679 ap_test= 0.66753 time= 1.82147\n",
      "Epoch: 0003 train_loss= 0.93083 val_roc= 0.69412 val_ap= 0.75048 test_roc= 0.65435 ap_test= 0.70079 time= 1.83336\n",
      "Epoch: 0004 train_loss= 0.73081 val_roc= 0.68001 val_ap= 0.73321 test_roc= 0.63894 ap_test= 0.68117 time= 1.82314\n",
      "Epoch: 0005 train_loss= 0.73560 val_roc= 0.68652 val_ap= 0.73672 test_roc= 0.64741 ap_test= 0.68578 time= 1.81945\n",
      "Epoch: 0006 train_loss= 0.73210 val_roc= 0.70106 val_ap= 0.74447 test_roc= 0.65923 ap_test= 0.69300 time= 1.81443\n",
      "Epoch: 0007 train_loss= 0.72771 val_roc= 0.71387 val_ap= 0.75141 test_roc= 0.67128 ap_test= 0.70089 time= 1.82910\n",
      "Epoch: 0008 train_loss= 0.72284 val_roc= 0.74392 val_ap= 0.77252 test_roc= 0.69233 ap_test= 0.71317 time= 1.80653\n",
      "Epoch: 0009 train_loss= 0.71206 val_roc= 0.76411 val_ap= 0.78212 test_roc= 0.69450 ap_test= 0.70986 time= 1.81627\n",
      "Epoch: 0010 train_loss= 0.69605 val_roc= 0.77550 val_ap= 0.79029 test_roc= 0.68517 ap_test= 0.70923 time= 1.82274\n",
      "Epoch: 0011 train_loss= 0.68384 val_roc= 0.80089 val_ap= 0.81155 test_roc= 0.72325 ap_test= 0.72255 time= 1.82495\n",
      "Epoch: 0012 train_loss= 0.66894 val_roc= 0.81022 val_ap= 0.81266 test_roc= 0.70350 ap_test= 0.71385 time= 1.83502\n",
      "Epoch: 0013 train_loss= 0.65470 val_roc= 0.80859 val_ap= 0.81179 test_roc= 0.70101 ap_test= 0.71955 time= 1.81360\n",
      "Epoch: 0014 train_loss= 0.64404 val_roc= 0.81380 val_ap= 0.81414 test_roc= 0.72032 ap_test= 0.73089 time= 1.81979\n",
      "Epoch: 0015 train_loss= 0.63650 val_roc= 0.81955 val_ap= 0.81857 test_roc= 0.71848 ap_test= 0.74457 time= 1.83173\n",
      "Epoch: 0016 train_loss= 0.62655 val_roc= 0.82997 val_ap= 0.83149 test_roc= 0.71946 ap_test= 0.75521 time= 1.82169\n",
      "Epoch: 0017 train_loss= 0.62035 val_roc= 0.83974 val_ap= 0.84521 test_roc= 0.72640 ap_test= 0.76172 time= 1.81978\n",
      "Epoch: 0018 train_loss= 0.61168 val_roc= 0.84299 val_ap= 0.84388 test_roc= 0.73942 ap_test= 0.77728 time= 1.81117\n",
      "Epoch: 0019 train_loss= 0.60637 val_roc= 0.83941 val_ap= 0.84280 test_roc= 0.73790 ap_test= 0.77763 time= 1.82311\n",
      "Epoch: 0020 train_loss= 0.60042 val_roc= 0.84852 val_ap= 0.85260 test_roc= 0.75255 ap_test= 0.79042 time= 1.81340\n",
      "Epoch: 0021 train_loss= 0.59394 val_roc= 0.85503 val_ap= 0.86084 test_roc= 0.76405 ap_test= 0.79816 time= 1.98674\n",
      "Epoch: 0022 train_loss= 0.58703 val_roc= 0.85438 val_ap= 0.86264 test_roc= 0.76752 ap_test= 0.80548 time= 2.02319\n",
      "Epoch: 0023 train_loss= 0.58308 val_roc= 0.86230 val_ap= 0.86915 test_roc= 0.77968 ap_test= 0.81378 time= 1.82050\n",
      "Epoch: 0024 train_loss= 0.57889 val_roc= 0.87467 val_ap= 0.88288 test_roc= 0.79541 ap_test= 0.82445 time= 1.80402\n",
      "Epoch: 0025 train_loss= 0.57344 val_roc= 0.87543 val_ap= 0.88487 test_roc= 0.80192 ap_test= 0.82673 time= 1.83685\n",
      "Epoch: 0026 train_loss= 0.56940 val_roc= 0.87522 val_ap= 0.88553 test_roc= 0.80745 ap_test= 0.83249 time= 1.80854\n",
      "Epoch: 0027 train_loss= 0.56407 val_roc= 0.88227 val_ap= 0.89263 test_roc= 0.81841 ap_test= 0.84006 time= 1.82019\n",
      "Epoch: 0028 train_loss= 0.56138 val_roc= 0.88498 val_ap= 0.89799 test_roc= 0.82525 ap_test= 0.84797 time= 1.82187\n",
      "Epoch: 0029 train_loss= 0.55450 val_roc= 0.88737 val_ap= 0.90088 test_roc= 0.83393 ap_test= 0.85422 time= 1.82699\n",
      "Epoch: 0030 train_loss= 0.55116 val_roc= 0.89638 val_ap= 0.90794 test_roc= 0.84272 ap_test= 0.85747 time= 1.81964\n",
      "Epoch: 0031 train_loss= 0.54536 val_roc= 0.89627 val_ap= 0.90897 test_roc= 0.84304 ap_test= 0.86033 time= 1.96208\n",
      "Epoch: 0032 train_loss= 0.54002 val_roc= 0.89572 val_ap= 0.91113 test_roc= 0.83979 ap_test= 0.86022 time= 1.82800\n",
      "Epoch: 0033 train_loss= 0.53681 val_roc= 0.90007 val_ap= 0.91609 test_roc= 0.85118 ap_test= 0.86537 time= 1.82252\n",
      "Epoch: 0034 train_loss= 0.53324 val_roc= 0.88965 val_ap= 0.91193 test_roc= 0.84489 ap_test= 0.86387 time= 1.81721\n",
      "Epoch: 0035 train_loss= 0.53073 val_roc= 0.89334 val_ap= 0.91528 test_roc= 0.85097 ap_test= 0.86870 time= 1.83648\n",
      "Epoch: 0036 train_loss= 0.52236 val_roc= 0.89714 val_ap= 0.91773 test_roc= 0.85303 ap_test= 0.87272 time= 1.94329\n",
      "Epoch: 0037 train_loss= 0.52032 val_roc= 0.88965 val_ap= 0.91512 test_roc= 0.85259 ap_test= 0.87344 time= 1.83854\n",
      "Epoch: 0038 train_loss= 0.51726 val_roc= 0.87804 val_ap= 0.91062 test_roc= 0.85270 ap_test= 0.87513 time= 1.83192\n",
      "Epoch: 0039 train_loss= 0.51550 val_roc= 0.87359 val_ap= 0.90774 test_roc= 0.85194 ap_test= 0.87538 time= 1.82076\n",
      "Epoch: 0040 train_loss= 0.51011 val_roc= 0.87760 val_ap= 0.91010 test_roc= 0.85509 ap_test= 0.87857 time= 1.94147\n",
      "Epoch: 0041 train_loss= 0.50782 val_roc= 0.88303 val_ap= 0.91216 test_roc= 0.85834 ap_test= 0.88084 time= 1.81321\n",
      "Epoch: 0042 train_loss= 0.50743 val_roc= 0.87272 val_ap= 0.90641 test_roc= 0.85292 ap_test= 0.87620 time= 1.82908\n",
      "Epoch: 0043 train_loss= 0.50314 val_roc= 0.86230 val_ap= 0.90251 test_roc= 0.85248 ap_test= 0.87974 time= 1.82446\n",
      "Epoch: 0044 train_loss= 0.50190 val_roc= 0.86122 val_ap= 0.90304 test_roc= 0.85314 ap_test= 0.88070 time= 1.82701\n",
      "Epoch: 0045 train_loss= 0.49774 val_roc= 0.86317 val_ap= 0.90328 test_roc= 0.85259 ap_test= 0.87955 time= 1.81139\n",
      "Epoch: 0046 train_loss= 0.49657 val_roc= 0.86230 val_ap= 0.90102 test_roc= 0.85693 ap_test= 0.88267 time= 1.80977\n",
      "Epoch: 0047 train_loss= 0.49686 val_roc= 0.85276 val_ap= 0.89606 test_roc= 0.85509 ap_test= 0.88227 time= 1.97800\n",
      "Epoch: 0048 train_loss= 0.49115 val_roc= 0.84896 val_ap= 0.89601 test_roc= 0.85292 ap_test= 0.88220 time= 1.82186\n",
      "Epoch: 0049 train_loss= 0.49339 val_roc= 0.85113 val_ap= 0.89679 test_roc= 0.85390 ap_test= 0.88283 time= 1.82163\n",
      "Epoch: 0050 train_loss= 0.48721 val_roc= 0.85113 val_ap= 0.89465 test_roc= 0.85672 ap_test= 0.88375 time= 1.81809\n",
      "Epoch: 0051 train_loss= 0.48625 val_roc= 0.85406 val_ap= 0.89741 test_roc= 0.85379 ap_test= 0.88167 time= 1.81551\n",
      "Epoch: 0052 train_loss= 0.48445 val_roc= 0.84679 val_ap= 0.89532 test_roc= 0.85704 ap_test= 0.88562 time= 1.83880\n",
      "Epoch: 0053 train_loss= 0.48086 val_roc= 0.84462 val_ap= 0.89280 test_roc= 0.85704 ap_test= 0.88621 time= 1.89208\n",
      "Epoch: 0054 train_loss= 0.48018 val_roc= 0.85243 val_ap= 0.89782 test_roc= 0.85487 ap_test= 0.88362 time= 1.90214\n",
      "Epoch: 0055 train_loss= 0.47849 val_roc= 0.84570 val_ap= 0.89433 test_roc= 0.85639 ap_test= 0.88456 time= 1.84052\n",
      "Epoch: 0056 train_loss= 0.47729 val_roc= 0.84950 val_ap= 0.89574 test_roc= 0.86106 ap_test= 0.88887 time= 1.82160\n",
      "Epoch: 0057 train_loss= 0.47403 val_roc= 0.84657 val_ap= 0.89405 test_roc= 0.86344 ap_test= 0.89087 time= 1.82005\n",
      "Epoch: 0058 train_loss= 0.47260 val_roc= 0.84505 val_ap= 0.89408 test_roc= 0.86095 ap_test= 0.88901 time= 2.00331\n",
      "Epoch: 0059 train_loss= 0.46986 val_roc= 0.84473 val_ap= 0.89386 test_roc= 0.86138 ap_test= 0.88870 time= 1.82435\n",
      "Epoch: 0060 train_loss= 0.46859 val_roc= 0.84592 val_ap= 0.89500 test_roc= 0.86366 ap_test= 0.89071 time= 1.81782\n",
      "Epoch: 0061 train_loss= 0.46640 val_roc= 0.84299 val_ap= 0.89467 test_roc= 0.86258 ap_test= 0.88932 time= 1.80815\n",
      "Epoch: 0062 train_loss= 0.46516 val_roc= 0.83713 val_ap= 0.89181 test_roc= 0.86279 ap_test= 0.88988 time= 1.81494\n",
      "Epoch: 0063 train_loss= 0.46368 val_roc= 0.84006 val_ap= 0.89192 test_roc= 0.86442 ap_test= 0.89104 time= 1.82464\n",
      "Epoch: 0064 train_loss= 0.46122 val_roc= 0.84288 val_ap= 0.89341 test_roc= 0.86095 ap_test= 0.88767 time= 1.98412\n",
      "Epoch: 0065 train_loss= 0.46036 val_roc= 0.83800 val_ap= 0.89236 test_roc= 0.86149 ap_test= 0.88825 time= 1.82291\n",
      "Epoch: 0066 train_loss= 0.45887 val_roc= 0.84017 val_ap= 0.89301 test_roc= 0.86334 ap_test= 0.88906 time= 2.18010\n",
      "Epoch: 0067 train_loss= 0.45885 val_roc= 0.83713 val_ap= 0.89219 test_roc= 0.85769 ap_test= 0.88609 time= 2.47538\n",
      "Epoch: 0068 train_loss= 0.45690 val_roc= 0.83442 val_ap= 0.88983 test_roc= 0.86030 ap_test= 0.89000 time= 2.19026\n",
      "Epoch: 0069 train_loss= 0.45453 val_roc= 0.83919 val_ap= 0.89013 test_roc= 0.86301 ap_test= 0.89084 time= 1.94794\n",
      "Epoch: 0070 train_loss= 0.45671 val_roc= 0.83464 val_ap= 0.88821 test_roc= 0.85379 ap_test= 0.88926 time= 1.97802\n",
      "Epoch: 0071 train_loss= 0.46206 val_roc= 0.83843 val_ap= 0.89094 test_roc= 0.85390 ap_test= 0.88490 time= 1.81720\n",
      "Epoch: 0072 train_loss= 0.45671 val_roc= 0.83453 val_ap= 0.89106 test_roc= 0.85726 ap_test= 0.88905 time= 1.83173\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 0073 train_loss= 0.45212 val_roc= 0.83637 val_ap= 0.88957 test_roc= 0.84945 ap_test= 0.88737 time= 1.88559\n",
      "Epoch: 0074 train_loss= 0.45658 val_roc= 0.83333 val_ap= 0.88941 test_roc= 0.84814 ap_test= 0.88158 time= 2.02899\n",
      "Epoch: 0075 train_loss= 0.44890 val_roc= 0.83778 val_ap= 0.89087 test_roc= 0.85683 ap_test= 0.88718 time= 1.85083\n",
      "Epoch: 0076 train_loss= 0.45104 val_roc= 0.83388 val_ap= 0.88856 test_roc= 0.84880 ap_test= 0.88489 time= 1.82686\n",
      "Epoch: 0077 train_loss= 0.45298 val_roc= 0.83496 val_ap= 0.89224 test_roc= 0.85173 ap_test= 0.88583 time= 1.83662\n",
      "Epoch: 0078 train_loss= 0.44575 val_roc= 0.84136 val_ap= 0.89387 test_roc= 0.85628 ap_test= 0.88766 time= 1.82362\n",
      "Epoch: 0079 train_loss= 0.44859 val_roc= 0.83138 val_ap= 0.88886 test_roc= 0.84576 ap_test= 0.88207 time= 1.82171\n",
      "Epoch: 0080 train_loss= 0.44395 val_roc= 0.82715 val_ap= 0.88640 test_roc= 0.84229 ap_test= 0.88163 time= 2.01004\n",
      "Epoch: 0081 train_loss= 0.44557 val_roc= 0.83887 val_ap= 0.89329 test_roc= 0.85303 ap_test= 0.88776 time= 1.85085\n",
      "Epoch: 0082 train_loss= 0.44279 val_roc= 0.83811 val_ap= 0.89299 test_roc= 0.85433 ap_test= 0.88525 time= 1.83483\n",
      "Epoch: 0083 train_loss= 0.44378 val_roc= 0.83171 val_ap= 0.88997 test_roc= 0.84066 ap_test= 0.87709 time= 1.83046\n",
      "Epoch: 0084 train_loss= 0.44224 val_roc= 0.82888 val_ap= 0.88889 test_roc= 0.84587 ap_test= 0.88314 time= 1.82285\n",
      "Epoch: 0085 train_loss= 0.44129 val_roc= 0.83171 val_ap= 0.89046 test_roc= 0.85878 ap_test= 0.89024 time= 1.82393\n",
      "Epoch: 0086 train_loss= 0.43965 val_roc= 0.83930 val_ap= 0.89454 test_roc= 0.84901 ap_test= 0.88241 time= 1.82907\n",
      "Epoch: 0087 train_loss= 0.43905 val_roc= 0.83550 val_ap= 0.89250 test_roc= 0.83990 ap_test= 0.87912 time= 1.90636\n",
      "Epoch: 0088 train_loss= 0.43887 val_roc= 0.82997 val_ap= 0.88949 test_roc= 0.84576 ap_test= 0.88500 time= 1.92087\n",
      "Optimization Finished!\n",
      "Stopped by Early Stopping method.\n",
      "\n",
      "Final Test ROC score: 0.85010\n",
      "Final Test AP score: 0.88742\n",
      "Time taken: 2m 46s\n",
      "\n",
      "Best test ROC: 0.86442 at iteration 62\n",
      "Best test AP: 0.89104 at iteration 62\n"
     ]
    }
   ],
   "source": [
    "# Initialize session\n",
    "sess = tf.Session()\n",
    "sess.run(tf.global_variables_initializer())\n",
    "\n",
    "adj_label = adj_train + sp.eye(adj_train.shape[0])\n",
    "adj_label = sparse_to_tuple(adj_label)\n",
    "\n",
    "best_test_roc = 0.\n",
    "best_roc_iter = 0\n",
    "best_test_ap = 0.\n",
    "best_ap_iter = 0\n",
    "\n",
    "epochs = 200\n",
    "early_stopping = True\n",
    "is_early_stopping = False\n",
    "patience = 25\n",
    "init_time = time.time()\n",
    "\n",
    "# Train model\n",
    "for epoch in range(epochs):\n",
    "    t = time.time()\n",
    "    # Construct feed dictionary\n",
    "    feed_dict = construct_feed_dict(adj_norm, adj_label, features, placeholders)\n",
    "    feed_dict.update({placeholders['dropout']: dropout})\n",
    "    # One update of parameter matrices\n",
    "    _, avg_cost = sess.run([opt.opt_op, opt.cost], feed_dict=feed_dict)\n",
    "    # Performance on validation set\n",
    "    roc_curr, ap_curr = get_roc_score(val_edges, val_edges_false)\n",
    "\n",
    "    roc_test, ap_test = get_roc_score(test_edges, test_edges_false)\n",
    "    \n",
    "    if best_test_roc < roc_test:\n",
    "        best_test_roc = roc_test\n",
    "        best_roc_iter = epoch\n",
    "    \n",
    "    if best_test_ap < ap_test:\n",
    "        best_test_ap = ap_test\n",
    "        best_ap_iter = epoch\n",
    "        \n",
    "    if early_stopping:\n",
    "        if epoch - best_roc_iter > patience:\n",
    "            is_early_stopping = True\n",
    "            break\n",
    "    \n",
    "    print(\"Epoch:\", '%04d' % (epoch + 1),\n",
    "          \"train_loss=\", \"{:.5f}\".format(avg_cost),\n",
    "          \"val_roc=\", \"{:.5f}\".format(roc_curr),\n",
    "          \"val_ap=\", \"{:.5f}\".format(ap_curr),\n",
    "          \"test_roc=\", \"{:.5f}\".format(roc_test),\n",
    "          \"ap_test=\", \"{:.5f}\".format(ap_test),\n",
    "          \"time=\", \"{:.5f}\".format(time.time() - t))\n",
    "\n",
    "print()\n",
    "print('Optimization Finished!')\n",
    "if is_early_stopping:\n",
    "    print (\"Stopped by Early Stopping method.\")\n",
    "print()\n",
    "\n",
    "roc_score, ap_score = get_roc_score(test_edges, test_edges_false)\n",
    "print('Final Test ROC score: {:.5f}'.format(roc_score))\n",
    "print('Final Test AP score: {:.5f}'.format(ap_score))\n",
    "print('Time taken: ' + secs_to_mins(time.time() - init_time))\n",
    "print()\n",
    "\n",
    "print('Best test ROC: {:.5f}'.format(best_test_roc), 'at iteration', best_roc_iter)\n",
    "print('Best test AP: {:.5f}'.format(best_test_ap), 'at iteration', best_ap_iter)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "graph[5], graph[6]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "u_feat = np.array(graph[5].todense())\n",
    "v_feat = np.array(graph[6].todense())\n",
    "\n",
    "padded_v = tf.keras.preprocessing.sequence.pad_sequences(v_feat, maxlen=u_feat.shape[1], dtype='float32', padding='post', truncating='post')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "features = np.concatenate([u_feat, padded_v], axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "features = tf.keras.preprocessing.sequence.pad_sequences(features, maxlen=u_feat.shape[0] + v_feat.shape[0], dtype='int64', padding='post', truncating='post')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "features = features[:-1,:-1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "adj_train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "features = sp.csr_matrix(features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'18s'"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "secs_to_mins(18.0202)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
