{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Variational Graph Auto-Encoder for DTI"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from __future__ import division\n",
    "from __future__ import print_function\n",
    "\n",
    "import os\n",
    "import time\n",
    "import pickle\n",
    "\n",
    "import numpy as np\n",
    "import scipy.sparse as sp\n",
    "\n",
    "import networkx as nx\n",
    "import tensorflow as tf\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# from plotly.offline import init_notebook_mode, iplot\n",
    "# from IPython.display import display, HTML\n",
    "\n",
    "# init_notebook_mode(connected=False)\n",
    "\n",
    "from datetime import datetime\n",
    "from collections import namedtuple\n",
    "from sklearn.metrics import roc_auc_score\n",
    "from sklearn.metrics import average_precision_score"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Utility functions**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def secs_to_mins(secs):\n",
    "    mins = int(secs / 60)\n",
    "    secs = int(secs % 60)\n",
    "    \n",
    "    if mins != 0:\n",
    "        time_in_mins = str(mins) + \"m \" + str(secs) + \"s\"\n",
    "    else:\n",
    "        time_in_mins = str(secs) + \"s\"\n",
    "        \n",
    "    return time_in_mins\n",
    "\n",
    "def build_tensorboard_dir():\n",
    "    dir_name = str(datetime.now())\n",
    "    dir_path = 'data/tensorboard/' + dir_name\n",
    "    \n",
    "    assert not os.path.exists(dir_path), 'Path already exists.'\n",
    "    os.makedirs(dir_path)\n",
    "\n",
    "def load_data():\n",
    "    g = nx.read_edgelist('yeast.edgelist')\n",
    "    adj = nx.adjacency_matrix(g)\n",
    "    return adj\n",
    "\n",
    "def glorot(shape, name=None):\n",
    "    \"\"\"Glorot & Bengio (AISTATS 2010) init.\"\"\"\n",
    "    init_range = np.sqrt(6.0/(shape[0]+shape[1]))\n",
    "    initial = tf.random_uniform(shape, minval=-init_range, maxval=init_range, dtype=tf.float32)\n",
    "    return tf.Variable(initial, name=name)\n",
    "\n",
    "def weight_variable_glorot(input_dim, output_dim, name=\"\"):\n",
    "    init_range = np.sqrt(6.0 / (input_dim + output_dim))\n",
    "    initial = tf.random_uniform(\n",
    "        [input_dim, output_dim], minval=-init_range,\n",
    "        maxval=init_range, dtype=tf.float32)\n",
    "    return tf.Variable(initial, name=name)\n",
    "\n",
    "def dropout_sparse(x, keep_prob, num_nonzero_elems):\n",
    "    noise_shape = [num_nonzero_elems]\n",
    "    random_tensor = keep_prob\n",
    "    random_tensor += tf.random_uniform(noise_shape)\n",
    "    dropout_mask = tf.cast(tf.floor(random_tensor), dtype=tf.bool)\n",
    "    pre_out = tf.sparse_retain(x, dropout_mask)\n",
    "    return pre_out * (1. / keep_prob)\n",
    "\n",
    "def sparse_to_tuple(sparse_mx):\n",
    "    if not sp.isspmatrix_coo(sparse_mx):\n",
    "        sparse_mx = sparse_mx.tocoo()\n",
    "    coords = np.vstack((sparse_mx.row, sparse_mx.col)).transpose()\n",
    "    values = sparse_mx.data\n",
    "    shape = sparse_mx.shape\n",
    "    return coords, values, shape\n",
    "\n",
    "def preprocess_graph(adj):\n",
    "    adj = sp.coo_matrix(adj)\n",
    "    adj_ = adj + sp.eye(adj.shape[0])\n",
    "    rowsum = np.array(adj_.sum(1))\n",
    "    degree_mat_inv_sqrt = sp.diags(np.power(rowsum, -0.5).flatten())\n",
    "    adj_normalized = adj_.dot(degree_mat_inv_sqrt).transpose().dot(degree_mat_inv_sqrt).tocoo()\n",
    "    return sparse_to_tuple(adj_normalized)\n",
    "\n",
    "def construct_feed_dict(adj_normalized, adj, features, placeholders):\n",
    "    feed_dict = dict()\n",
    "    feed_dict.update({placeholders['features']: features})\n",
    "    feed_dict.update({placeholders['adj']: adj_normalized})\n",
    "    feed_dict.update({placeholders['adj_orig']: adj})\n",
    "    return feed_dict\n",
    "\n",
    "def mask_test_edges(adj):\n",
    "    # Function to build test set with 2% positive links\n",
    "    # Remove diagonal elements\n",
    "    adj = adj - sp.dia_matrix((adj.diagonal()[np.newaxis, :], [0]), shape=adj.shape)\n",
    "    adj.eliminate_zeros()\n",
    "\n",
    "    adj_triu = sp.triu(adj)\n",
    "    adj_tuple = sparse_to_tuple(adj_triu)\n",
    "    edges = adj_tuple[0]\n",
    "    edges_all = sparse_to_tuple(adj)[0]\n",
    "    num_test = int(np.floor(edges.shape[0] / 50.))\n",
    "    num_val = int(np.floor(edges.shape[0] / 50.))\n",
    "\n",
    "    all_edge_idx = range(edges.shape[0])\n",
    "    np.random.shuffle(all_edge_idx)\n",
    "    val_edge_idx = all_edge_idx[:num_val]\n",
    "    test_edge_idx = all_edge_idx[num_val:(num_val + num_test)]\n",
    "    test_edges = edges[test_edge_idx]\n",
    "    val_edges = edges[val_edge_idx]\n",
    "    train_edges = np.delete(edges, np.hstack([test_edge_idx, val_edge_idx]), axis=0)\n",
    "\n",
    "    def ismember(a, b):\n",
    "        rows_close = np.all((a - b[:, None]) == 0, axis=-1)\n",
    "        return np.any(rows_close)\n",
    "\n",
    "    test_edges_false = []\n",
    "    while len(test_edges_false) < len(test_edges):\n",
    "        n_rnd = len(test_edges) - len(test_edges_false)\n",
    "        rnd = np.random.randint(0, adj.shape[0], size=2 * n_rnd)\n",
    "        idxs_i = rnd[:n_rnd]                                        \n",
    "        idxs_j = rnd[n_rnd:]\n",
    "        for i in range(n_rnd):\n",
    "            idx_i = idxs_i[i]\n",
    "            idx_j = idxs_j[i]\n",
    "            if idx_i == idx_j:\n",
    "                continue\n",
    "            if ismember([idx_i, idx_j], edges_all):\n",
    "                continue\n",
    "            if test_edges_false:\n",
    "                if ismember([idx_j, idx_i], np.array(test_edges_false)):\n",
    "                    continue\n",
    "                if ismember([idx_i, idx_j], np.array(test_edges_false)):\n",
    "                    continue\n",
    "            test_edges_false.append([idx_i, idx_j])\n",
    "\n",
    "    val_edges_false = []\n",
    "    while len(val_edges_false) < len(val_edges):\n",
    "        n_rnd = len(val_edges) - len(val_edges_false)\n",
    "        rnd = np.random.randint(0, adj.shape[0], size=2 * n_rnd)\n",
    "        idxs_i = rnd[:n_rnd]                                        \n",
    "        idxs_j = rnd[n_rnd:]\n",
    "        for i in range(n_rnd):\n",
    "            idx_i = idxs_i[i]\n",
    "            idx_j = idxs_j[i]\n",
    "            if idx_i == idx_j:\n",
    "                continue\n",
    "            if ismember([idx_i, idx_j], train_edges):\n",
    "                continue\n",
    "            if ismember([idx_j, idx_i], train_edges):\n",
    "                continue\n",
    "            if ismember([idx_i, idx_j], val_edges):\n",
    "                continue\n",
    "            if ismember([idx_j, idx_i], val_edges):\n",
    "                continue\n",
    "            if val_edges_false:\n",
    "                if ismember([idx_j, idx_i], np.array(val_edges_false)):\n",
    "                    continue\n",
    "                if ismember([idx_i, idx_j], np.array(val_edges_false)):\n",
    "                    continue\n",
    "            val_edges_false.append([idx_i, idx_j])\n",
    "\n",
    "    # Re-build adj matrix\n",
    "    data = np.ones(train_edges.shape[0])\n",
    "    adj_train = sp.csr_matrix((data, (train_edges[:, 0], train_edges[:, 1])), shape=adj.shape)\n",
    "    adj_train = adj_train + adj_train.T\n",
    "\n",
    "    return adj_train, train_edges, val_edges, val_edges_false, test_edges, test_edges_false\n",
    "\n",
    "\n",
    "def get_roc_score(edges_pos, edges_neg):\n",
    "    feed_dict.update({placeholders['dropout']: 0})\n",
    "    emb = sess.run(model.embeddings, feed_dict=feed_dict)\n",
    "\n",
    "    def sigmoid(x):\n",
    "        return 1 / (1 + np.exp(-x))\n",
    "\n",
    "    # Predict on test set of edges\n",
    "    adj_rec = np.dot(emb, emb.T)\n",
    "    preds = []\n",
    "    pos = []\n",
    "    for e in edges_pos:\n",
    "        preds.append(sigmoid(adj_rec[e[0], e[1]]))\n",
    "        pos.append(adj_orig[e[0], e[1]])\n",
    "\n",
    "    preds_neg = []\n",
    "    neg = []\n",
    "    for e in edges_neg:\n",
    "        preds_neg.append(sigmoid(adj_rec[e[0], e[1]]))\n",
    "        neg.append(adj_orig[e[0], e[1]])\n",
    "\n",
    "    preds_all = np.hstack([preds, preds_neg])\n",
    "    labels_all = np.hstack([np.ones(len(preds)), np.zeros(len(preds))])\n",
    "    roc_score = roc_auc_score(labels_all, preds_all)\n",
    "    ap_score = average_precision_score(labels_all, preds_all)\n",
    "\n",
    "    return roc_score, ap_score"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Layer Definitions**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Layer(object):\n",
    "    \"\"\"Base layer class. Defines basic API for all layer objects.\n",
    "    Implementation inspired by keras (http://keras.io).\n",
    "\n",
    "    # Properties\n",
    "        name: String, defines the variable scope of the layer.\n",
    "        logging: Boolean, switches Tensorflow histogram logging on/off\n",
    "\n",
    "    # Methods\n",
    "        _call(inputs): Defines computation graph of layer\n",
    "            (i.e. takes input, returns output)\n",
    "        __call__(inputs): Wrapper for _call()\n",
    "        _log_vars(): Log all variables\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, **kwargs):\n",
    "        allowed_kwargs = {'name', 'logging'}\n",
    "        for kwarg in kwargs.keys():\n",
    "            assert kwarg in allowed_kwargs, 'Invalid keyword argument: ' + kwarg\n",
    "        name = kwargs.get('name')\n",
    "        if not name:\n",
    "            layer = self.__class__.__name__.lower()\n",
    "            name = layer + '_' + str(get_layer_uid(layer))\n",
    "        self.name = name\n",
    "        self.vars = {}\n",
    "        logging = kwargs.get('logging', False)\n",
    "        self.logging = logging\n",
    "        self.sparse_inputs = False\n",
    "\n",
    "    def _call(self, inputs):\n",
    "        return inputs\n",
    "\n",
    "    def __call__(self, inputs):\n",
    "        with tf.name_scope(self.name):\n",
    "            if self.logging and not self.sparse_inputs:\n",
    "                tf.summary.histogram(self.name + '/inputs', inputs)\n",
    "            outputs = self._call(inputs)\n",
    "            if self.logging:\n",
    "                tf.summary.histogram(self.name + '/outputs', outputs)\n",
    "            return outputs\n",
    "\n",
    "    def _log_vars(self):\n",
    "        for var in self.vars:\n",
    "            tf.summary.histogram(self.name + '/vars/' + var, self.vars[var])\n",
    "\n",
    "\n",
    "class GraphConvolution():\n",
    "    \"\"\"Basic graph convolution layer for undirected graph without edge labels.\"\"\"\n",
    "    def __init__(self, input_dim, output_dim, adj, name, dropout=0., act=tf.nn.relu):\n",
    "        self.name = name\n",
    "        self.vars = {}\n",
    "        self.issparse = False\n",
    "        with tf.variable_scope(self.name + '_vars'):\n",
    "            self.vars['weights'] = weight_variable_glorot(input_dim, output_dim, name='weights')\n",
    "        self.dropout = dropout\n",
    "        self.adj = adj\n",
    "        self.act = act\n",
    "\n",
    "    def __call__(self, inputs):\n",
    "        with tf.name_scope(self.name):        \n",
    "            x = inputs\n",
    "            x = tf.nn.dropout(x, 1-self.dropout)\n",
    "            x = tf.matmul(x, self.vars['weights'])\n",
    "            x = tf.sparse_tensor_dense_matmul(self.adj, x)\n",
    "            outputs = self.act(x)\n",
    "        return outputs\n",
    "\n",
    "\n",
    "class GraphConvolutionSparse():\n",
    "    \"\"\"Graph convolution layer for sparse inputs.\"\"\"\n",
    "    def __init__(self, input_dim, output_dim, adj, features_nonzero, name, dropout=0., act=tf.nn.relu):\n",
    "        self.name = name\n",
    "        self.vars = {}\n",
    "        self.issparse = False\n",
    "        with tf.variable_scope(self.name + '_vars'):\n",
    "            self.vars['weights'] = weight_variable_glorot(input_dim, output_dim, name='weights')\n",
    "        self.dropout = dropout\n",
    "        self.adj = adj\n",
    "        self.act = act\n",
    "        self.issparse = True\n",
    "        self.features_nonzero = features_nonzero\n",
    "\n",
    "    def __call__(self, inputs):\n",
    "        with tf.name_scope(self.name):\n",
    "            x = inputs\n",
    "            x = dropout_sparse(x, 1-self.dropout, self.features_nonzero)\n",
    "            x = tf.sparse_tensor_dense_matmul(x, self.vars['weights'])\n",
    "            x = tf.sparse_tensor_dense_matmul(self.adj, x)\n",
    "            outputs = self.act(x)\n",
    "        return outputs\n",
    "    \n",
    "def dot(x, y, sparse=False):\n",
    "    \"\"\"Wrapper for tf.matmul (sparse vs dense).\"\"\"\n",
    "    if sparse:\n",
    "        res = tf.sparse_tensor_dense_matmul(x, y)\n",
    "    else:\n",
    "        res = tf.matmul(x, y)\n",
    "    return res\n",
    "    \n",
    "class Dense(Layer):\n",
    "    \"\"\"Dense layer.\"\"\"\n",
    "    def __init__(self, input_dim, output_dim, placeholders, dropout=0., sparse_inputs=False,\n",
    "                 act=tf.nn.relu, bias=False, featureless=False, **kwargs):\n",
    "        super(Dense, self).__init__(**kwargs)\n",
    "\n",
    "        if dropout is not None:\n",
    "            self.dropout = placeholders['dropout']\n",
    "        else:\n",
    "            self.dropout = 0.\n",
    "\n",
    "        self.act = act\n",
    "        self.sparse_inputs = sparse_inputs\n",
    "        self.featureless = featureless\n",
    "        self.bias = bias\n",
    "\n",
    "        # helper variable for sparse dropout\n",
    "        self.num_features_nonzero = placeholders['num_features_nonzero']\n",
    "\n",
    "        with tf.variable_scope(self.name + '_vars'):\n",
    "            self.vars['weights'] = weight_variable_glorot(input_dim, output_dim,\n",
    "                                          name='weights')\n",
    "            if self.bias:\n",
    "                self.vars['bias'] = zeros([output_dim], name='bias')\n",
    "\n",
    "        if self.logging:\n",
    "            self._log_vars()\n",
    "\n",
    "    def _call(self, inputs):\n",
    "        x = inputs\n",
    "\n",
    "        # dropout\n",
    "        if self.sparse_inputs:\n",
    "            x = sparse_dropout(x, 1-self.dropout, self.num_features_nonzero)\n",
    "        else:\n",
    "            x = tf.nn.dropout(x, 1-self.dropout)\n",
    "\n",
    "        # transform\n",
    "        output = dot(x, self.vars['weights'], sparse=self.sparse_inputs)\n",
    "\n",
    "        # bias\n",
    "        if self.bias:\n",
    "            output += self.vars['bias']\n",
    "\n",
    "        return self.act(output)\n",
    "    \n",
    "class InnerProductDecoder():\n",
    "    \"\"\"Decoder model layer for link prediction.\"\"\"\n",
    "    def __init__(self, input_dim, name, dropout=0., act=tf.nn.sigmoid):\n",
    "        self.name = name\n",
    "        self.issparse = False\n",
    "        self.dropout = dropout\n",
    "        self.act = act\n",
    "\n",
    "    def __call__(self, inputs):\n",
    "        with tf.name_scope(self.name):\n",
    "            inputs = tf.nn.dropout(inputs, 1-self.dropout)\n",
    "            x = tf.transpose(inputs)\n",
    "            x = tf.matmul(inputs, x)\n",
    "            x = tf.reshape(x, [-1])\n",
    "            outputs = self.act(x)\n",
    "        return outputs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Loading Data**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('data/dti_store/graph_2.pkl') as f:\n",
    "    graph = pickle.load(f)\n",
    "    \n",
    "u_nodes, v_nodes, ratings = graph[2], graph[3], graph[4]\n",
    "g = np.vstack([u_nodes, v_nodes, ratings])\n",
    "\n",
    "g = g.T\n",
    "g = g[g[:, 2].argsort()]\n",
    "g_0, g_1 = np.split(g, np.where(np.diff(g[:,2]))[0]+1)\n",
    "\n",
    "graph_pos = g_1\n",
    "graph_pos = graph_pos.T\n",
    "\n",
    "x, y, _ = graph_pos\n",
    "\n",
    "x = x.reshape((1, x.shape[0]))\n",
    "y = y.reshape((1, y.shape[0]))\n",
    "graph_pos = np.concatenate([x, y], axis=0)\n",
    "\n",
    "graph_pos = graph_pos.T\n",
    "\n",
    "for i in range(len(graph_pos)):\n",
    "    graph_pos[i][1] += 1861\n",
    "    \n",
    "adj = list()\n",
    "for s, d in graph_pos:\n",
    "    adj.append((s, d))\n",
    "    \n",
    "G = nx.Graph(adj)\n",
    "adj = nx.adj_matrix(G)\n",
    "\n",
    "num_nodes = adj.shape[0]\n",
    "num_edges = adj.sum()\n",
    "\n",
    "# # Featureless\n",
    "u_feat = np.array(graph[5].todense())\n",
    "v_feat = np.array(graph[6].todense())\n",
    "\n",
    "padded_v = tf.keras.preprocessing.sequence.pad_sequences(v_feat, maxlen=u_feat.shape[1], dtype='float32', padding='post', truncating='post')\n",
    "features = np.concatenate([u_feat, padded_v], axis=0)\n",
    "# features = tf.keras.preprocessing.sequence.pad_sequences(features, maxlen=u_feat.shape[0] + v_feat.shape[0], dtype='int64', padding='post', truncating='post')\n",
    "features = features[:-1,:-1]\n",
    "\n",
    "features = sp.csr_matrix(features)\n",
    "\n",
    "features = sparse_to_tuple(features)\n",
    "num_features = features[2][1]\n",
    "features_nonzero = features[1].shape[0]\n",
    "\n",
    "# Store original adjacency matrix (without diagonal entries) for later\n",
    "adj_orig = adj - sp.dia_matrix((adj.diagonal()[np.newaxis, :], [0]), shape=adj.shape)\n",
    "adj_orig.eliminate_zeros()\n",
    "\n",
    "adj_train, train_edges, val_edges, val_edges_false, test_edges, test_edges_false = mask_test_edges(adj)\n",
    "adj = adj_train\n",
    "\n",
    "adj_norm = preprocess_graph(adj)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Hyperparameters**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set random seed\n",
    "seed = 123\n",
    "np.random.seed(seed)\n",
    "tf.set_random_seed(seed)\n",
    "\n",
    "# Learning rate\n",
    "learning_rate = 0.001\n",
    "\n",
    "# Layer sizes\n",
    "hidden1 = 512\n",
    "hidden2 = 256\n",
    "hidden3 = 128\n",
    "hidden4 = 64\n",
    "hidden5 = 32\n",
    "hidden6 = 64\n",
    "\n",
    "# drop_prob\n",
    "dropout = 0.1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "validation_auroc_readings = []"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Building the model**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "class VariationalGraphAutoEncoder:\n",
    "    \n",
    "    def __init__(self, placeholders, num_features, features_nonzero, num_nodes, name):\n",
    "        \"\"\" Class for modelling a Variational graph auto-encoder. \"\"\"\n",
    "        \n",
    "        self.name = name\n",
    "        self.placeholders = placeholders\n",
    "        self.inputs = placeholders['features']\n",
    "        self.input_dim = num_features\n",
    "        self.features_nonzero = features_nonzero\n",
    "        self.adj = placeholders['adj']\n",
    "        self.dropout = placeholders['dropout']\n",
    "        self.n_samples = num_nodes\n",
    "        with tf.variable_scope(self.name):\n",
    "            self.build()\n",
    "        \n",
    "    def build(self):\n",
    "        \"\"\" Build the architecture. \"\"\"\n",
    "        \n",
    "        # Set random seed\n",
    "        seed = 1324\n",
    "        np.random.seed(seed)\n",
    "        tf.set_random_seed(seed)\n",
    "\n",
    "        # Learning rate\n",
    "        learning_rate = 0.001\n",
    "\n",
    "        # Layer sizes\n",
    "        hidden1 = 512\n",
    "        hidden2 = 256\n",
    "        hidden3 = 128\n",
    "        hidden4 = 64\n",
    "        hidden5 = 32\n",
    "        hidden6 = 64\n",
    "\n",
    "        # drop_prob\n",
    "        dropout = 0.1\n",
    "        \n",
    "        self.hidden1 = GraphConvolutionSparse(\n",
    "            name='vgae_graph_conv_sparse_1',\n",
    "            input_dim=self.input_dim,\n",
    "            output_dim=hidden1,\n",
    "            adj=self.adj,\n",
    "            features_nonzero=self.features_nonzero,\n",
    "            act=tf.nn.relu,\n",
    "            dropout=self.dropout)(self.inputs)\n",
    "                \n",
    "        self.z_mean = GraphConvolution(\n",
    "            name='mean_layer',\n",
    "            input_dim=hidden1,\n",
    "            output_dim=hidden2,\n",
    "            adj=self.adj,\n",
    "            act=lambda x: x,\n",
    "            dropout=self.dropout)(self.hidden1)\n",
    "\n",
    "        self.z_log_std = GraphConvolution(\n",
    "            name='std_dev_layer',\n",
    "            input_dim=hidden1,\n",
    "            output_dim=hidden2,\n",
    "            adj=self.adj,\n",
    "            act=lambda x: x,\n",
    "            dropout=self.dropout)(self.hidden1)\n",
    "\n",
    "        self.embeddings = self.z_mean + tf.random_normal([self.n_samples, hidden2]) * tf.exp(self.z_log_std)\n",
    "        \n",
    "#         self.hidden2 = GraphConvolution(\n",
    "#             name='vgae_graph_conv_1',\n",
    "#             input_dim=hidden1,\n",
    "#             output_dim=hidden2,\n",
    "#             adj=self.adj,\n",
    "#             act=tf.nn.relu,\n",
    "#             dropout=self.dropout)(self.hidden1)\n",
    "        \n",
    "#         self.hidden3 = Dense(\n",
    "#             name='vgae_dense_1',\n",
    "#             input_dim=hidden2,\n",
    "#             output_dim=hidden3,\n",
    "#             placeholders=self.placeholders,\n",
    "#             act=tf.nn.relu,\n",
    "#             dropout=self.dropout)(self.z)\n",
    "        \n",
    "#         self.hidden4 = Dense(\n",
    "#             name='vgae_dense_2',\n",
    "#             input_dim=hidden3,\n",
    "#             output_dim=hidden4,\n",
    "#             placeholders=self.placeholders,\n",
    "#             act=tf.nn.relu,\n",
    "#             dropout=self.dropout)(self.hidden3)\n",
    "\n",
    "#         self.embeddings = Dense(\n",
    "#             name='vgae_dense_3',\n",
    "#             input_dim=hidden4,\n",
    "#             output_dim=hidden5,\n",
    "#             placeholders=self.placeholders,\n",
    "#             act=lambda x: x,\n",
    "#             dropout=self.dropout)(self.hidden4)\n",
    "    \n",
    "        self.hidden5 = Dense(\n",
    "            name='vgae_dense_4',\n",
    "            input_dim=hidden2,\n",
    "            output_dim=hidden3,\n",
    "            placeholders=self.placeholders,\n",
    "            act=lambda x: x,\n",
    "            dropout=self.dropout)(self.embeddings)\n",
    "        \n",
    "        self.reconstructions = InnerProductDecoder(\n",
    "            name='vgae_inner_product_1',\n",
    "            input_dim=hidden3,\n",
    "            act=lambda x: x)(self.hidden5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Optimizer():\n",
    "    \n",
    "    def __init__(self, preds, labels, num_nodes, num_edges):\n",
    "        \"\"\" Defines optimizer class for the VGAE model. \"\"\"\n",
    "        \n",
    "        pos_weight = float(num_nodes**2) / num_edges\n",
    "        norm = num_nodes**2 / float((num_nodes**2 - num_edges) * 2)\n",
    "        \n",
    "        preds_sub = preds\n",
    "        labels_sub = labels\n",
    "        \n",
    "        self.cost = norm * tf.reduce_mean(\n",
    "            tf.nn.weighted_cross_entropy_with_logits(\n",
    "                logits=preds_sub, targets=labels_sub, pos_weight=pos_weight))\n",
    "        self.optimizer = tf.train.AdamOptimizer(learning_rate=learning_rate)\n",
    "        \n",
    "        self.opt_op = self.optimizer.minimize(self.cost)\n",
    "        self.grads_vars = self.optimizer.compute_gradients(self.cost)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define placeholders\n",
    "placeholders = {\n",
    "    'features': tf.sparse_placeholder(tf.float32),\n",
    "    'adj': tf.sparse_placeholder(tf.float32),\n",
    "    'adj_orig': tf.sparse_placeholder(tf.float32),\n",
    "    'dropout': tf.placeholder_with_default(0., shape=()),\n",
    "    'num_features_nonzero': tf.placeholder(tf.int32)\n",
    "}\n",
    "\n",
    "# Create model\n",
    "model = VariationalGraphAutoEncoder(placeholders, num_features, features_nonzero, adj.shape[0], name='dti_gcn')\n",
    "\n",
    "# Create optimizer\n",
    "with tf.name_scope('optimizer'):\n",
    "    opt = Optimizer(\n",
    "        preds=model.reconstructions,\n",
    "        labels=tf.reshape(tf.sparse_tensor_to_dense(placeholders['adj_orig'], validate_indices=False), [-1]),\n",
    "        num_nodes=num_nodes,\n",
    "        num_edges=num_edges)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Training**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 0001 train_loss= 8.28216 val_roc= 0.58952 val_ap= 0.57473 test_roc= 0.60683 ap_test= 0.58514 time= 2.90656\n",
      "Epoch: 0002 train_loss= 6.81621 val_roc= 0.62288 val_ap= 0.63288 test_roc= 0.61350 ap_test= 0.63742 time= 1.35214\n",
      "Epoch: 0003 train_loss= 5.97853 val_roc= 0.65734 val_ap= 0.70831 test_roc= 0.67307 ap_test= 0.70648 time= 1.34348\n",
      "Epoch: 0004 train_loss= 5.13572 val_roc= 0.62717 val_ap= 0.69337 test_roc= 0.64095 ap_test= 0.70498 time= 1.34891\n",
      "Epoch: 0005 train_loss= 4.36431 val_roc= 0.66439 val_ap= 0.73007 test_roc= 0.66265 ap_test= 0.72330 time= 1.35559\n",
      "Epoch: 0006 train_loss= 3.54903 val_roc= 0.65191 val_ap= 0.71795 test_roc= 0.67828 ap_test= 0.73706 time= 1.34926\n",
      "Epoch: 0007 train_loss= 2.94390 val_roc= 0.68349 val_ap= 0.74889 test_roc= 0.68755 ap_test= 0.74399 time= 1.34498\n",
      "Epoch: 0008 train_loss= 2.42766 val_roc= 0.72656 val_ap= 0.74698 test_roc= 0.68007 ap_test= 0.72087 time= 1.36159\n",
      "Epoch: 0009 train_loss= 2.06091 val_roc= 0.68012 val_ap= 0.71898 test_roc= 0.66558 ap_test= 0.71378 time= 1.36125\n",
      "Epoch: 0010 train_loss= 1.72604 val_roc= 0.67367 val_ap= 0.72478 test_roc= 0.69488 ap_test= 0.73379 time= 1.52953\n",
      "Epoch: 0011 train_loss= 1.52820 val_roc= 0.70605 val_ap= 0.73126 test_roc= 0.68859 ap_test= 0.73473 time= 1.37651\n",
      "Epoch: 0012 train_loss= 1.34973 val_roc= 0.70171 val_ap= 0.73262 test_roc= 0.69466 ap_test= 0.73618 time= 1.35792\n",
      "Epoch: 0013 train_loss= 1.25804 val_roc= 0.68842 val_ap= 0.71781 test_roc= 0.67556 ap_test= 0.73163 time= 1.34398\n",
      "Epoch: 0014 train_loss= 1.17630 val_roc= 0.70161 val_ap= 0.72814 test_roc= 0.70085 ap_test= 0.74159 time= 1.35956\n",
      "Epoch: 0015 train_loss= 1.06964 val_roc= 0.67166 val_ap= 0.71427 test_roc= 0.67839 ap_test= 0.72812 time= 1.35854\n",
      "Epoch: 0016 train_loss= 1.03005 val_roc= 0.69189 val_ap= 0.72892 test_roc= 0.66829 ap_test= 0.71996 time= 1.35792\n",
      "Epoch: 0017 train_loss= 0.96573 val_roc= 0.68066 val_ap= 0.73271 test_roc= 0.68142 ap_test= 0.72193 time= 1.36136\n",
      "Epoch: 0018 train_loss= 0.91863 val_roc= 0.68468 val_ap= 0.74259 test_roc= 0.67589 ap_test= 0.71439 time= 1.36344\n",
      "Epoch: 0019 train_loss= 0.89049 val_roc= 0.67779 val_ap= 0.75019 test_roc= 0.67367 ap_test= 0.73148 time= 1.36614\n",
      "Epoch: 0020 train_loss= 0.85371 val_roc= 0.67307 val_ap= 0.74363 test_roc= 0.68012 ap_test= 0.74018 time= 1.36620\n",
      "Epoch: 0021 train_loss= 0.82386 val_roc= 0.70508 val_ap= 0.76423 test_roc= 0.67600 ap_test= 0.74261 time= 1.38269\n",
      "Epoch: 0022 train_loss= 0.79279 val_roc= 0.70345 val_ap= 0.76178 test_roc= 0.71495 ap_test= 0.76225 time= 1.37207\n",
      "Epoch: 0023 train_loss= 0.78442 val_roc= 0.70508 val_ap= 0.76189 test_roc= 0.71235 ap_test= 0.76149 time= 1.37376\n",
      "Epoch: 0024 train_loss= 0.75181 val_roc= 0.73524 val_ap= 0.78020 test_roc= 0.69727 ap_test= 0.75468 time= 1.38002\n",
      "Epoch: 0025 train_loss= 0.73345 val_roc= 0.70150 val_ap= 0.76345 test_roc= 0.67936 ap_test= 0.74492 time= 1.36512\n",
      "Epoch: 0026 train_loss= 0.72495 val_roc= 0.73362 val_ap= 0.78951 test_roc= 0.68544 ap_test= 0.75530 time= 1.64322\n",
      "Epoch: 0027 train_loss= 0.70110 val_roc= 0.68934 val_ap= 0.75525 test_roc= 0.70443 ap_test= 0.76426 time= 1.44048\n",
      "Epoch: 0028 train_loss= 0.71275 val_roc= 0.75358 val_ap= 0.79952 test_roc= 0.70085 ap_test= 0.76423 time= 1.44037\n",
      "Epoch: 0029 train_loss= 0.70422 val_roc= 0.71137 val_ap= 0.78537 test_roc= 0.70692 ap_test= 0.76855 time= 1.38317\n",
      "Epoch: 0030 train_loss= 0.68548 val_roc= 0.78602 val_ap= 0.82610 test_roc= 0.74110 ap_test= 0.78649 time= 1.37710\n",
      "Epoch: 0031 train_loss= 0.68029 val_roc= 0.74110 val_ap= 0.80826 test_roc= 0.73741 ap_test= 0.78132 time= 1.36187\n",
      "Epoch: 0032 train_loss= 0.66715 val_roc= 0.73958 val_ap= 0.79977 test_roc= 0.72797 ap_test= 0.77783 time= 1.37358\n",
      "Epoch: 0033 train_loss= 0.66569 val_roc= 0.78754 val_ap= 0.79808 test_roc= 0.74327 ap_test= 0.79159 time= 1.37053\n",
      "Epoch: 0034 train_loss= 0.66143 val_roc= 0.78027 val_ap= 0.82014 test_roc= 0.73231 ap_test= 0.76596 time= 1.37644\n",
      "Epoch: 0035 train_loss= 0.66167 val_roc= 0.76042 val_ap= 0.80793 test_roc= 0.76790 ap_test= 0.76245 time= 1.37329\n",
      "Epoch: 0036 train_loss= 0.64285 val_roc= 0.75716 val_ap= 0.81202 test_roc= 0.75781 ap_test= 0.79192 time= 1.38309\n",
      "Epoch: 0037 train_loss= 0.65222 val_roc= 0.79015 val_ap= 0.84105 test_roc= 0.76573 ap_test= 0.80209 time= 1.37319\n",
      "Epoch: 0038 train_loss= 0.64584 val_roc= 0.77170 val_ap= 0.82121 test_roc= 0.73720 ap_test= 0.78760 time= 1.36743\n",
      "Epoch: 0039 train_loss= 0.63778 val_roc= 0.80122 val_ap= 0.83355 test_roc= 0.78830 ap_test= 0.80994 time= 1.38458\n",
      "Epoch: 0040 train_loss= 0.64592 val_roc= 0.80414 val_ap= 0.81704 test_roc= 0.77246 ap_test= 0.81360 time= 1.38525\n",
      "Epoch: 0041 train_loss= 0.63311 val_roc= 0.79134 val_ap= 0.82455 test_roc= 0.75933 ap_test= 0.77803 time= 1.44036\n",
      "Epoch: 0042 train_loss= 0.62857 val_roc= 0.80306 val_ap= 0.84356 test_roc= 0.77550 ap_test= 0.82421 time= 1.46059\n",
      "Epoch: 0043 train_loss= 0.63090 val_roc= 0.78700 val_ap= 0.80646 test_roc= 0.72700 ap_test= 0.76031 time= 1.38087\n",
      "Epoch: 0044 train_loss= 0.61253 val_roc= 0.80621 val_ap= 0.84632 test_roc= 0.78635 ap_test= 0.80898 time= 1.37890\n",
      "Epoch: 0045 train_loss= 0.62165 val_roc= 0.77658 val_ap= 0.80922 test_roc= 0.78906 ap_test= 0.82135 time= 1.36450\n",
      "Epoch: 0046 train_loss= 0.62133 val_roc= 0.76118 val_ap= 0.82052 test_roc= 0.76107 ap_test= 0.79805 time= 1.38064\n",
      "Epoch: 0047 train_loss= 0.62431 val_roc= 0.80339 val_ap= 0.84850 test_roc= 0.79134 ap_test= 0.82256 time= 1.37030\n",
      "Epoch: 0048 train_loss= 0.60595 val_roc= 0.78516 val_ap= 0.83460 test_roc= 0.76866 ap_test= 0.81075 time= 1.38091\n",
      "Epoch: 0049 train_loss= 0.62053 val_roc= 0.83203 val_ap= 0.86960 test_roc= 0.79395 ap_test= 0.81511 time= 1.36678\n",
      "Epoch: 0050 train_loss= 0.60206 val_roc= 0.78570 val_ap= 0.80189 test_roc= 0.76660 ap_test= 0.80463 time= 1.37145\n",
      "Epoch: 0051 train_loss= 0.61436 val_roc= 0.80838 val_ap= 0.85166 test_roc= 0.77582 ap_test= 0.80241 time= 1.37139\n",
      "Epoch: 0052 train_loss= 0.59988 val_roc= 0.81022 val_ap= 0.84419 test_roc= 0.78472 ap_test= 0.81226 time= 1.82896\n",
      "Epoch: 0053 train_loss= 0.59596 val_roc= 0.83539 val_ap= 0.85957 test_roc= 0.79601 ap_test= 0.83165 time= 1.38929\n",
      "Epoch: 0054 train_loss= 0.59564 val_roc= 0.81901 val_ap= 0.85848 test_roc= 0.76931 ap_test= 0.80229 time= 1.38828\n",
      "Epoch: 0055 train_loss= 0.59330 val_roc= 0.77734 val_ap= 0.83148 test_roc= 0.78548 ap_test= 0.80612 time= 1.45885\n",
      "Epoch: 0056 train_loss= 0.58300 val_roc= 0.81521 val_ap= 0.84598 test_roc= 0.74251 ap_test= 0.77325 time= 1.38920\n",
      "Epoch: 0057 train_loss= 0.60059 val_roc= 0.78440 val_ap= 0.78927 test_roc= 0.79156 ap_test= 0.81015 time= 1.39533\n",
      "Epoch: 0058 train_loss= 0.58453 val_roc= 0.84646 val_ap= 0.87980 test_roc= 0.77116 ap_test= 0.80733 time= 1.39770\n",
      "Epoch: 0059 train_loss= 0.58083 val_roc= 0.81434 val_ap= 0.85750 test_roc= 0.78375 ap_test= 0.80400 time= 1.36634\n",
      "Epoch: 0060 train_loss= 0.57715 val_roc= 0.79015 val_ap= 0.83709 test_roc= 0.78103 ap_test= 0.81364 time= 1.35225\n",
      "Epoch: 0061 train_loss= 0.57585 val_roc= 0.83290 val_ap= 0.86965 test_roc= 0.78440 ap_test= 0.82829 time= 1.38085\n",
      "Epoch: 0062 train_loss= 0.57625 val_roc= 0.82726 val_ap= 0.86141 test_roc= 0.78179 ap_test= 0.81655 time= 1.37549\n",
      "Epoch: 0063 train_loss= 0.57409 val_roc= 0.79850 val_ap= 0.80295 test_roc= 0.77962 ap_test= 0.81710 time= 1.38889\n",
      "Epoch: 0064 train_loss= 0.57746 val_roc= 0.83171 val_ap= 0.86226 test_roc= 0.81304 ap_test= 0.84310 time= 1.37796\n",
      "Epoch: 0065 train_loss= 0.56867 val_roc= 0.80946 val_ap= 0.84545 test_roc= 0.78852 ap_test= 0.81490 time= 1.41759\n",
      "Epoch: 0066 train_loss= 0.57869 val_roc= 0.82628 val_ap= 0.85310 test_roc= 0.79156 ap_test= 0.82085 time= 1.40104\n",
      "Epoch: 0067 train_loss= 0.56874 val_roc= 0.81717 val_ap= 0.86518 test_roc= 0.78505 ap_test= 0.82484 time= 1.38289\n",
      "Epoch: 0068 train_loss= 0.57517 val_roc= 0.82378 val_ap= 0.84880 test_roc= 0.78223 ap_test= 0.80813 time= 1.37825\n",
      "Epoch: 0069 train_loss= 0.57005 val_roc= 0.82378 val_ap= 0.84546 test_roc= 0.75098 ap_test= 0.78258 time= 1.36565\n",
      "Epoch: 0070 train_loss= 0.56588 val_roc= 0.80458 val_ap= 0.83117 test_roc= 0.77745 ap_test= 0.80466 time= 1.38693\n",
      "Epoch: 0071 train_loss= 0.56829 val_roc= 0.83637 val_ap= 0.86465 test_roc= 0.79123 ap_test= 0.81840 time= 1.38745\n",
      "Epoch: 0072 train_loss= 0.56155 val_roc= 0.84776 val_ap= 0.86989 test_roc= 0.80859 ap_test= 0.83456 time= 1.59251\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 0073 train_loss= 0.55325 val_roc= 0.82585 val_ap= 0.85839 test_roc= 0.82813 ap_test= 0.84910 time= 1.37874\n",
      "Epoch: 0074 train_loss= 0.56300 val_roc= 0.82281 val_ap= 0.86050 test_roc= 0.77452 ap_test= 0.81831 time= 1.37013\n",
      "Epoch: 0075 train_loss= 0.55777 val_roc= 0.83930 val_ap= 0.84888 test_roc= 0.81283 ap_test= 0.83165 time= 1.37749\n",
      "Epoch: 0076 train_loss= 0.56035 val_roc= 0.83095 val_ap= 0.87155 test_roc= 0.79080 ap_test= 0.82898 time= 1.36433\n",
      "Epoch: 0077 train_loss= 0.54810 val_roc= 0.83713 val_ap= 0.86512 test_roc= 0.79731 ap_test= 0.81319 time= 1.37364\n",
      "Epoch: 0078 train_loss= 0.55879 val_roc= 0.83301 val_ap= 0.86298 test_roc= 0.76953 ap_test= 0.80824 time= 1.41490\n",
      "Epoch: 0079 train_loss= 0.55429 val_roc= 0.83409 val_ap= 0.85156 test_roc= 0.82444 ap_test= 0.84758 time= 1.37033\n",
      "Epoch: 0080 train_loss= 0.54723 val_roc= 0.82834 val_ap= 0.87010 test_roc= 0.80881 ap_test= 0.83918 time= 1.35804\n",
      "Epoch: 0081 train_loss= 0.55154 val_roc= 0.85243 val_ap= 0.88868 test_roc= 0.79915 ap_test= 0.82717 time= 1.37354\n",
      "Epoch: 0082 train_loss= 0.54515 val_roc= 0.85916 val_ap= 0.88204 test_roc= 0.81022 ap_test= 0.84265 time= 1.37542\n",
      "Epoch: 0083 train_loss= 0.55080 val_roc= 0.87609 val_ap= 0.89762 test_roc= 0.81391 ap_test= 0.84037 time= 1.36297\n",
      "Epoch: 0084 train_loss= 0.54644 val_roc= 0.81977 val_ap= 0.85835 test_roc= 0.83203 ap_test= 0.86127 time= 1.38119\n",
      "Epoch: 0085 train_loss= 0.54767 val_roc= 0.86317 val_ap= 0.88316 test_roc= 0.83746 ap_test= 0.85345 time= 1.37820\n",
      "Epoch: 0086 train_loss= 0.53784 val_roc= 0.81738 val_ap= 0.84919 test_roc= 0.81337 ap_test= 0.83005 time= 1.36783\n",
      "Epoch: 0087 train_loss= 0.54087 val_roc= 0.82498 val_ap= 0.85920 test_roc= 0.81022 ap_test= 0.83377 time= 1.37940\n",
      "Epoch: 0088 train_loss= 0.53878 val_roc= 0.86111 val_ap= 0.88936 test_roc= 0.79069 ap_test= 0.83041 time= 1.37639\n",
      "Epoch: 0089 train_loss= 0.53767 val_roc= 0.83225 val_ap= 0.87986 test_roc= 0.81261 ap_test= 0.84089 time= 1.38319\n",
      "Epoch: 0090 train_loss= 0.53554 val_roc= 0.83648 val_ap= 0.87821 test_roc= 0.80979 ap_test= 0.84196 time= 1.37030\n",
      "Epoch: 0091 train_loss= 0.53982 val_roc= 0.86784 val_ap= 0.89543 test_roc= 0.78961 ap_test= 0.81745 time= 1.36062\n",
      "Epoch: 0092 train_loss= 0.53364 val_roc= 0.86079 val_ap= 0.89609 test_roc= 0.78841 ap_test= 0.82998 time= 1.36775\n",
      "Epoch: 0093 train_loss= 0.53389 val_roc= 0.84418 val_ap= 0.86719 test_roc= 0.81478 ap_test= 0.84134 time= 1.38210\n",
      "Epoch: 0094 train_loss= 0.52943 val_roc= 0.85558 val_ap= 0.86927 test_roc= 0.82064 ap_test= 0.84225 time= 1.36908\n",
      "Epoch: 0095 train_loss= 0.52926 val_roc= 0.85286 val_ap= 0.87432 test_roc= 0.82791 ap_test= 0.85679 time= 1.37083\n",
      "Epoch: 0096 train_loss= 0.52787 val_roc= 0.85623 val_ap= 0.86136 test_roc= 0.80490 ap_test= 0.84444 time= 1.37863\n",
      "Epoch: 0097 train_loss= 0.52320 val_roc= 0.88401 val_ap= 0.90783 test_roc= 0.83073 ap_test= 0.85898 time= 1.37057\n",
      "Epoch: 0098 train_loss= 0.52670 val_roc= 0.84006 val_ap= 0.84799 test_roc= 0.81793 ap_test= 0.84723 time= 1.36746\n",
      "Epoch: 0099 train_loss= 0.52179 val_roc= 0.85558 val_ap= 0.89100 test_roc= 0.80642 ap_test= 0.82871 time= 1.36398\n",
      "Epoch: 0100 train_loss= 0.51855 val_roc= 0.87120 val_ap= 0.89598 test_roc= 0.80838 ap_test= 0.84702 time= 1.38802\n",
      "Epoch: 0101 train_loss= 0.52064 val_roc= 0.84896 val_ap= 0.88330 test_roc= 0.82194 ap_test= 0.85613 time= 1.44501\n",
      "Epoch: 0102 train_loss= 0.52103 val_roc= 0.84950 val_ap= 0.89365 test_roc= 0.81239 ap_test= 0.84214 time= 1.36860\n",
      "Epoch: 0103 train_loss= 0.52093 val_roc= 0.83431 val_ap= 0.86448 test_roc= 0.82552 ap_test= 0.85198 time= 1.38422\n",
      "Epoch: 0104 train_loss= 0.52092 val_roc= 0.89193 val_ap= 0.91259 test_roc= 0.85265 ap_test= 0.87624 time= 1.38166\n",
      "Epoch: 0105 train_loss= 0.51936 val_roc= 0.86003 val_ap= 0.89518 test_roc= 0.84028 ap_test= 0.85271 time= 1.37372\n",
      "Epoch: 0106 train_loss= 0.52041 val_roc= 0.88151 val_ap= 0.90912 test_roc= 0.80903 ap_test= 0.84628 time= 1.37594\n",
      "Epoch: 0107 train_loss= 0.51519 val_roc= 0.87196 val_ap= 0.89878 test_roc= 0.81782 ap_test= 0.85010 time= 1.40114\n",
      "Epoch: 0108 train_loss= 0.50973 val_roc= 0.86274 val_ap= 0.87877 test_roc= 0.80165 ap_test= 0.83852 time= 1.36178\n",
      "Epoch: 0109 train_loss= 0.51705 val_roc= 0.86599 val_ap= 0.89845 test_roc= 0.82454 ap_test= 0.85727 time= 1.35965\n",
      "Epoch: 0110 train_loss= 0.51578 val_roc= 0.86480 val_ap= 0.90084 test_roc= 0.82031 ap_test= 0.85278 time= 1.34634\n",
      "Epoch: 0111 train_loss= 0.50923 val_roc= 0.83388 val_ap= 0.87201 test_roc= 0.81597 ap_test= 0.84021 time= 1.36317\n",
      "Epoch: 0112 train_loss= 0.51274 val_roc= 0.86426 val_ap= 0.90118 test_roc= 0.81966 ap_test= 0.84492 time= 1.35203\n",
      "Epoch: 0113 train_loss= 0.51023 val_roc= 0.87489 val_ap= 0.90879 test_roc= 0.81261 ap_test= 0.85431 time= 1.35580\n",
      "Epoch: 0114 train_loss= 0.50538 val_roc= 0.84744 val_ap= 0.89487 test_roc= 0.82693 ap_test= 0.85183 time= 1.35743\n",
      "Epoch: 0115 train_loss= 0.50374 val_roc= 0.84581 val_ap= 0.87936 test_roc= 0.83735 ap_test= 0.86168 time= 1.35654\n",
      "Epoch: 0116 train_loss= 0.50694 val_roc= 0.84462 val_ap= 0.88190 test_roc= 0.84776 ap_test= 0.87289 time= 1.34832\n",
      "Epoch: 0117 train_loss= 0.49954 val_roc= 0.87229 val_ap= 0.90522 test_roc= 0.83496 ap_test= 0.86188 time= 1.35893\n",
      "Epoch: 0118 train_loss= 0.49849 val_roc= 0.87023 val_ap= 0.89998 test_roc= 0.83713 ap_test= 0.85448 time= 1.35242\n",
      "Epoch: 0119 train_loss= 0.49944 val_roc= 0.86936 val_ap= 0.89811 test_roc= 0.83518 ap_test= 0.84026 time= 1.36065\n",
      "Epoch: 0120 train_loss= 0.49443 val_roc= 0.86339 val_ap= 0.90193 test_roc= 0.82997 ap_test= 0.85003 time= 1.35095\n",
      "Epoch: 0121 train_loss= 0.49120 val_roc= 0.86567 val_ap= 0.89499 test_roc= 0.85102 ap_test= 0.87474 time= 1.35601\n",
      "Epoch: 0122 train_loss= 0.49491 val_roc= 0.87554 val_ap= 0.91200 test_roc= 0.81055 ap_test= 0.85049 time= 1.36375\n",
      "Epoch: 0123 train_loss= 0.48967 val_roc= 0.86903 val_ap= 0.90310 test_roc= 0.81055 ap_test= 0.84734 time= 1.35542\n",
      "Epoch: 0124 train_loss= 0.49406 val_roc= 0.85688 val_ap= 0.89906 test_roc= 0.80686 ap_test= 0.84526 time= 1.34218\n",
      "Epoch: 0125 train_loss= 0.49165 val_roc= 0.87587 val_ap= 0.88634 test_roc= 0.82161 ap_test= 0.85644 time= 1.35870\n",
      "Epoch: 0126 train_loss= 0.48909 val_roc= 0.87554 val_ap= 0.90880 test_roc= 0.81576 ap_test= 0.83864 time= 1.36799\n",
      "Epoch: 0127 train_loss= 0.49066 val_roc= 0.87001 val_ap= 0.90030 test_roc= 0.83518 ap_test= 0.85801 time= 1.44620\n",
      "Epoch: 0128 train_loss= 0.48791 val_roc= 0.87446 val_ap= 0.89870 test_roc= 0.85438 ap_test= 0.87914 time= 1.35822\n",
      "Epoch: 0129 train_loss= 0.48924 val_roc= 0.87598 val_ap= 0.89486 test_roc= 0.82151 ap_test= 0.83957 time= 1.42354\n",
      "Epoch: 0130 train_loss= 0.49325 val_roc= 0.85851 val_ap= 0.89948 test_roc= 0.84874 ap_test= 0.87234 time= 1.36591\n",
      "Epoch: 0131 train_loss= 0.48865 val_roc= 0.88422 val_ap= 0.91105 test_roc= 0.85319 ap_test= 0.88165 time= 1.46585\n",
      "Epoch: 0132 train_loss= 0.48870 val_roc= 0.85232 val_ap= 0.88756 test_roc= 0.86057 ap_test= 0.88574 time= 1.87140\n",
      "Epoch: 0133 train_loss= 0.49110 val_roc= 0.86968 val_ap= 0.90383 test_roc= 0.83518 ap_test= 0.86391 time= 1.53144\n",
      "Epoch: 0134 train_loss= 0.48753 val_roc= 0.87185 val_ap= 0.90423 test_roc= 0.85829 ap_test= 0.87734 time= 1.36820\n",
      "Epoch: 0135 train_loss= 0.48553 val_roc= 0.87316 val_ap= 0.90687 test_roc= 0.83887 ap_test= 0.86943 time= 1.36597\n",
      "Epoch: 0136 train_loss= 0.48597 val_roc= 0.88672 val_ap= 0.91579 test_roc= 0.84527 ap_test= 0.87467 time= 1.37032\n",
      "Epoch: 0137 train_loss= 0.48785 val_roc= 0.88704 val_ap= 0.91758 test_roc= 0.84082 ap_test= 0.86575 time= 1.35451\n",
      "Epoch: 0138 train_loss= 0.48327 val_roc= 0.84798 val_ap= 0.89009 test_roc= 0.84993 ap_test= 0.87977 time= 1.38227\n",
      "Epoch: 0139 train_loss= 0.49201 val_roc= 0.88346 val_ap= 0.91040 test_roc= 0.85720 ap_test= 0.87764 time= 1.82395\n",
      "Epoch: 0140 train_loss= 0.48254 val_roc= 0.87815 val_ap= 0.91552 test_roc= 0.83279 ap_test= 0.86871 time= 2.12475\n",
      "Epoch: 0141 train_loss= 0.48172 val_roc= 0.88737 val_ap= 0.91378 test_roc= 0.82422 ap_test= 0.84082 time= 1.68249\n",
      "Epoch: 0142 train_loss= 0.48398 val_roc= 0.88379 val_ap= 0.91293 test_roc= 0.84581 ap_test= 0.87572 time= 1.70814\n",
      "Epoch: 0143 train_loss= 0.47850 val_roc= 0.88270 val_ap= 0.91248 test_roc= 0.84039 ap_test= 0.86496 time= 1.48590\n",
      "Epoch: 0144 train_loss= 0.48042 val_roc= 0.88954 val_ap= 0.92172 test_roc= 0.84234 ap_test= 0.87141 time= 1.45137\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 0145 train_loss= 0.47799 val_roc= 0.88889 val_ap= 0.91714 test_roc= 0.83073 ap_test= 0.87138 time= 1.56184\n",
      "Epoch: 0146 train_loss= 0.47799 val_roc= 0.87055 val_ap= 0.90571 test_roc= 0.84256 ap_test= 0.86624 time= 1.36762\n",
      "Epoch: 0147 train_loss= 0.47832 val_roc= 0.88357 val_ap= 0.91489 test_roc= 0.84473 ap_test= 0.86123 time= 1.38257\n",
      "Epoch: 0148 train_loss= 0.47538 val_roc= 0.90180 val_ap= 0.92843 test_roc= 0.86502 ap_test= 0.87427 time= 1.38783\n",
      "Epoch: 0149 train_loss= 0.47804 val_roc= 0.88770 val_ap= 0.91396 test_roc= 0.83908 ap_test= 0.87194 time= 1.36815\n",
      "Epoch: 0150 train_loss= 0.47224 val_roc= 0.87294 val_ap= 0.88920 test_roc= 0.85786 ap_test= 0.88706 time= 1.37889\n",
      "Epoch: 0151 train_loss= 0.47462 val_roc= 0.87305 val_ap= 0.91354 test_roc= 0.83735 ap_test= 0.86985 time= 1.34947\n",
      "Epoch: 0152 train_loss= 0.47387 val_roc= 0.88270 val_ap= 0.91584 test_roc= 0.83930 ap_test= 0.87070 time= 1.36986\n",
      "Epoch: 0153 train_loss= 0.46881 val_roc= 0.86686 val_ap= 0.90720 test_roc= 0.83789 ap_test= 0.86377 time= 2.31750\n",
      "Epoch: 0154 train_loss= 0.47449 val_roc= 0.87326 val_ap= 0.90995 test_roc= 0.82802 ap_test= 0.86879 time= 2.12720\n",
      "Epoch: 0155 train_loss= 0.47058 val_roc= 0.85178 val_ap= 0.90297 test_roc= 0.81977 ap_test= 0.86045 time= 2.30388\n",
      "Epoch: 0156 train_loss= 0.47405 val_roc= 0.86480 val_ap= 0.87013 test_roc= 0.83312 ap_test= 0.87027 time= 1.76523\n",
      "Epoch: 0157 train_loss= 0.47042 val_roc= 0.87109 val_ap= 0.90629 test_roc= 0.84874 ap_test= 0.87670 time= 1.37117\n",
      "Epoch: 0158 train_loss= 0.46864 val_roc= 0.85536 val_ap= 0.90035 test_roc= 0.85710 ap_test= 0.87547 time= 1.51877\n",
      "Epoch: 0159 train_loss= 0.47476 val_roc= 0.87164 val_ap= 0.91014 test_roc= 0.82780 ap_test= 0.86138 time= 1.51290\n",
      "Epoch: 0160 train_loss= 0.46413 val_roc= 0.88032 val_ap= 0.89997 test_roc= 0.84776 ap_test= 0.87911 time= 1.51065\n",
      "Epoch: 0161 train_loss= 0.47096 val_roc= 0.87912 val_ap= 0.91415 test_roc= 0.84342 ap_test= 0.87099 time= 1.36680\n",
      "Epoch: 0162 train_loss= 0.46962 val_roc= 0.89931 val_ap= 0.92538 test_roc= 0.84191 ap_test= 0.86740 time= 1.37802\n",
      "Epoch: 0163 train_loss= 0.47331 val_roc= 0.88466 val_ap= 0.91493 test_roc= 0.83138 ap_test= 0.87010 time= 1.98512\n",
      "Epoch: 0164 train_loss= 0.46194 val_roc= 0.88043 val_ap= 0.91642 test_roc= 0.83181 ap_test= 0.86572 time= 1.63044\n",
      "Epoch: 0165 train_loss= 0.46946 val_roc= 0.89030 val_ap= 0.92262 test_roc= 0.82682 ap_test= 0.86181 time= 1.36017\n",
      "Epoch: 0166 train_loss= 0.46927 val_roc= 0.88900 val_ap= 0.91575 test_roc= 0.83301 ap_test= 0.86497 time= 1.52016\n",
      "Epoch: 0167 train_loss= 0.47128 val_roc= 0.87294 val_ap= 0.91273 test_roc= 0.84592 ap_test= 0.87838 time= 1.39103\n",
      "Epoch: 0168 train_loss= 0.46618 val_roc= 0.85438 val_ap= 0.89593 test_roc= 0.83529 ap_test= 0.87272 time= 1.40919\n",
      "Epoch: 0169 train_loss= 0.48071 val_roc= 0.87240 val_ap= 0.87917 test_roc= 0.85319 ap_test= 0.88327 time= 1.47372\n",
      "Epoch: 0170 train_loss= 0.46150 val_roc= 0.86317 val_ap= 0.88506 test_roc= 0.81239 ap_test= 0.85499 time= 1.91283\n",
      "Epoch: 0171 train_loss= 0.47328 val_roc= 0.88401 val_ap= 0.91504 test_roc= 0.86979 ap_test= 0.89233 time= 1.49062\n",
      "Epoch: 0172 train_loss= 0.46513 val_roc= 0.86871 val_ap= 0.90331 test_roc= 0.82541 ap_test= 0.85865 time= 1.60224\n",
      "Epoch: 0173 train_loss= 0.46760 val_roc= 0.87099 val_ap= 0.89876 test_roc= 0.85634 ap_test= 0.88446 time= 1.43898\n",
      "Epoch: 0174 train_loss= 0.46754 val_roc= 0.87500 val_ap= 0.90918 test_roc= 0.84429 ap_test= 0.87828 time= 1.36411\n",
      "Epoch: 0175 train_loss= 0.46791 val_roc= 0.87402 val_ap= 0.90600 test_roc= 0.84256 ap_test= 0.87663 time= 1.43542\n",
      "Epoch: 0176 train_loss= 0.47048 val_roc= 0.89681 val_ap= 0.92556 test_roc= 0.85862 ap_test= 0.88834 time= 1.61235\n",
      "Epoch: 0177 train_loss= 0.46176 val_roc= 0.89355 val_ap= 0.91141 test_roc= 0.85449 ap_test= 0.87771 time= 1.71269\n",
      "Epoch: 0178 train_loss= 0.47499 val_roc= 0.88694 val_ap= 0.92002 test_roc= 0.84993 ap_test= 0.87740 time= 1.79874\n",
      "Epoch: 0179 train_loss= 0.45936 val_roc= 0.87956 val_ap= 0.91546 test_roc= 0.84538 ap_test= 0.87368 time= 1.72935\n",
      "Epoch: 0180 train_loss= 0.46765 val_roc= 0.89019 val_ap= 0.92249 test_roc= 0.83843 ap_test= 0.86777 time= 1.92283\n",
      "Epoch: 0181 train_loss= 0.46016 val_roc= 0.90441 val_ap= 0.92936 test_roc= 0.83811 ap_test= 0.87305 time= 1.45729\n",
      "Epoch: 0182 train_loss= 0.45932 val_roc= 0.87934 val_ap= 0.91798 test_roc= 0.81630 ap_test= 0.85139 time= 1.40046\n",
      "Epoch: 0183 train_loss= 0.46421 val_roc= 0.90148 val_ap= 0.92737 test_roc= 0.84180 ap_test= 0.88059 time= 1.39910\n",
      "Epoch: 0184 train_loss= 0.45841 val_roc= 0.87348 val_ap= 0.91380 test_roc= 0.84581 ap_test= 0.87548 time= 1.47245\n",
      "Epoch: 0185 train_loss= 0.46330 val_roc= 0.87457 val_ap= 0.89411 test_roc= 0.85200 ap_test= 0.87933 time= 1.42641\n",
      "Epoch: 0186 train_loss= 0.45943 val_roc= 0.87077 val_ap= 0.90503 test_roc= 0.85905 ap_test= 0.88971 time= 1.40754\n",
      "Epoch: 0187 train_loss= 0.45834 val_roc= 0.88097 val_ap= 0.91096 test_roc= 0.84429 ap_test= 0.88045 time= 1.38703\n",
      "Epoch: 0188 train_loss= 0.45971 val_roc= 0.88086 val_ap= 0.91926 test_roc= 0.85731 ap_test= 0.88708 time= 1.40135\n",
      "Epoch: 0189 train_loss= 0.45954 val_roc= 0.89703 val_ap= 0.92554 test_roc= 0.83366 ap_test= 0.86732 time= 1.38236\n",
      "Epoch: 0190 train_loss= 0.45605 val_roc= 0.88553 val_ap= 0.91757 test_roc= 0.83887 ap_test= 0.87448 time= 1.38648\n",
      "Epoch: 0191 train_loss= 0.46220 val_roc= 0.87847 val_ap= 0.90749 test_roc= 0.84874 ap_test= 0.88230 time= 1.37291\n",
      "Epoch: 0192 train_loss= 0.45693 val_roc= 0.87695 val_ap= 0.90417 test_roc= 0.84353 ap_test= 0.87861 time= 1.38500\n",
      "Epoch: 0193 train_loss= 0.45987 val_roc= 0.87728 val_ap= 0.91638 test_roc= 0.85449 ap_test= 0.87564 time= 1.37402\n",
      "Epoch: 0194 train_loss= 0.45753 val_roc= 0.88205 val_ap= 0.91074 test_roc= 0.84614 ap_test= 0.87592 time= 1.39247\n",
      "Epoch: 0195 train_loss= 0.45512 val_roc= 0.84798 val_ap= 0.88753 test_roc= 0.82248 ap_test= 0.86688 time= 1.38592\n",
      "Epoch: 0196 train_loss= 0.45692 val_roc= 0.89193 val_ap= 0.92493 test_roc= 0.86480 ap_test= 0.89460 time= 1.39804\n",
      "Epoch: 0197 train_loss= 0.45530 val_roc= 0.89301 val_ap= 0.92329 test_roc= 0.86426 ap_test= 0.88710 time= 1.38897\n",
      "Epoch: 0198 train_loss= 0.45918 val_roc= 0.87413 val_ap= 0.91204 test_roc= 0.85569 ap_test= 0.88322 time= 1.38718\n",
      "Epoch: 0199 train_loss= 0.45192 val_roc= 0.87109 val_ap= 0.89426 test_roc= 0.83974 ap_test= 0.87295 time= 1.37653\n",
      "Epoch: 0200 train_loss= 0.45734 val_roc= 0.87478 val_ap= 0.91291 test_roc= 0.85981 ap_test= 0.88476 time= 1.38521\n",
      "\n",
      "Optimization Finished!\n",
      "\n",
      "Final Test ROC score: 0.84635\n",
      "Final Test AP score: 0.87756\n",
      "Time taken: 4m 49s\n",
      "\n",
      "Best test ROC: 0.86979 at iteration 170\n",
      "Best test AP: 0.89460 at iteration 195\n"
     ]
    }
   ],
   "source": [
    "# Initialize session\n",
    "sess = tf.Session()\n",
    "sess.run(tf.global_variables_initializer())\n",
    "\n",
    "adj_label = adj_train + sp.eye(adj_train.shape[0])\n",
    "adj_label = sparse_to_tuple(adj_label)\n",
    "\n",
    "best_test_roc = 0.\n",
    "best_roc_iter = 0\n",
    "best_test_ap = 0.\n",
    "best_ap_iter = 0\n",
    "\n",
    "epochs = 200\n",
    "early_stopping = False\n",
    "is_early_stopping = False\n",
    "patience = 25\n",
    "init_time = time.time()\n",
    "\n",
    "val_rocs = []\n",
    "\n",
    "# Train model\n",
    "for epoch in range(epochs):\n",
    "    t = time.time()\n",
    "    # Construct feed dictionary\n",
    "    feed_dict = construct_feed_dict(adj_norm, adj_label, features, placeholders)\n",
    "    feed_dict.update({placeholders['dropout']: dropout})\n",
    "    # One update of parameter matrices\n",
    "    _, avg_cost = sess.run([opt.opt_op, opt.cost], feed_dict=feed_dict)\n",
    "    # Performance on validation set\n",
    "    roc_curr, ap_curr = get_roc_score(val_edges, val_edges_false)\n",
    "    roc_test, ap_test = get_roc_score(test_edges, test_edges_false)\n",
    "    \n",
    "    val_rocs.append(roc_curr)\n",
    "    \n",
    "    if best_test_roc < roc_test:\n",
    "        best_test_roc = roc_test\n",
    "        best_roc_iter = epoch\n",
    "    \n",
    "    if best_test_ap < ap_test:\n",
    "        best_test_ap = ap_test\n",
    "        best_ap_iter = epoch\n",
    "        \n",
    "    if early_stopping:\n",
    "        if epoch - best_roc_iter > patience:\n",
    "            is_early_stopping = True\n",
    "            break\n",
    "    \n",
    "    print(\"Epoch:\", '%04d' % (epoch + 1),\n",
    "          \"train_loss=\", \"{:.5f}\".format(avg_cost),\n",
    "          \"val_roc=\", \"{:.5f}\".format(roc_curr),\n",
    "          \"val_ap=\", \"{:.5f}\".format(ap_curr),\n",
    "          \"test_roc=\", \"{:.5f}\".format(roc_test),\n",
    "          \"ap_test=\", \"{:.5f}\".format(ap_test),\n",
    "          \"time=\", \"{:.5f}\".format(time.time() - t))\n",
    "\n",
    "print()\n",
    "print('Optimization Finished!')\n",
    "if is_early_stopping:\n",
    "    print (\"Stopped by Early Stopping method.\")\n",
    "print()\n",
    "\n",
    "roc_score, ap_score = get_roc_score(test_edges, test_edges_false)\n",
    "print('Final Test ROC score: {:.5f}'.format(roc_score))\n",
    "print('Final Test AP score: {:.5f}'.format(ap_score))\n",
    "print('Time taken: ' + secs_to_mins(time.time() - init_time))\n",
    "print()\n",
    "\n",
    "print('Best test ROC: {:.5f}'.format(best_test_roc), 'at iteration', best_roc_iter)\n",
    "print('Best test AP: {:.5f}'.format(best_test_ap), 'at iteration', best_ap_iter)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "val_rocs = np.array(val_rocs)\n",
    "epochs = np.arange(0., len(val_rocs), 1.)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.plot(epochs, val_rocs)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "validation_auroc_readings.append(val_rocs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[array([0.47591146, 0.56141493, 0.63487413, 0.61328125, 0.60134549,\n",
       "        0.6110026 , 0.60959201, 0.59874132, 0.54383681, 0.54134115,\n",
       "        0.5327691 , 0.5609809 , 0.52148438, 0.54953342, 0.49734158,\n",
       "        0.54079861, 0.59809028, 0.63650174, 0.65722656, 0.68088108,\n",
       "        0.60177951, 0.65332031, 0.60232205, 0.62196181, 0.59233941,\n",
       "        0.69737413, 0.65972222, 0.66818576, 0.63302951, 0.63682726,\n",
       "        0.64322917, 0.6703559 , 0.63226997, 0.61924913, 0.67198351,\n",
       "        0.63085938, 0.68272569, 0.66861979, 0.67664931, 0.65983073,\n",
       "        0.68500434, 0.6500651 , 0.69184028, 0.65896267, 0.69422743,\n",
       "        0.6890191 , 0.71571181, 0.6984592 , 0.66297743, 0.71527778,\n",
       "        0.70844184, 0.68630642, 0.69357639, 0.70214844, 0.73014323,\n",
       "        0.71907552, 0.69639757, 0.6859809 , 0.69542101, 0.70062934,\n",
       "        0.72927517, 0.7358941 , 0.71614583, 0.74012587, 0.72135417,\n",
       "        0.69401042, 0.7046441 , 0.70019531, 0.67361111, 0.70562066,\n",
       "        0.73394097, 0.71940104, 0.75130208, 0.71766493, 0.70583767,\n",
       "        0.71451823, 0.75379774, 0.70247396, 0.75303819, 0.75444878,\n",
       "        0.72862413, 0.7234158 , 0.73491753, 0.74186198, 0.75217014,\n",
       "        0.7562934 , 0.78483073, 0.76953125, 0.77571615, 0.77734375,\n",
       "        0.78081597, 0.7562934 , 0.77300347, 0.79720052, 0.7796224 ,\n",
       "        0.7797309 , 0.80523003, 0.7796224 , 0.78103299, 0.77517361,\n",
       "        0.80110677, 0.81054688, 0.75954861, 0.7937283 , 0.79947917,\n",
       "        0.81911892, 0.77528212, 0.77940538, 0.77235243, 0.79785156,\n",
       "        0.79188368, 0.78993056, 0.80957031, 0.78569878, 0.78342014,\n",
       "        0.77864583, 0.8219401 , 0.82942708, 0.82736545, 0.7874349 ,\n",
       "        0.80056424, 0.80132378, 0.81195747, 0.80609809, 0.79351128,\n",
       "        0.7843967 , 0.82736545, 0.79589844, 0.8264974 , 0.83702257,\n",
       "        0.82584635, 0.81152344, 0.81575521, 0.79904514, 0.82486979,\n",
       "        0.82009549, 0.82454427, 0.82237413, 0.84960938, 0.82953559,\n",
       "        0.84288194, 0.78732639, 0.80457899, 0.84733073, 0.81206597,\n",
       "        0.87467448, 0.82486979, 0.84396701, 0.81825087, 0.81065538,\n",
       "        0.84928385, 0.83897569, 0.84418403, 0.85720486, 0.84027778,\n",
       "        0.85210503, 0.83865017, 0.86414931, 0.86111111, 0.85590278,\n",
       "        0.84342448, 0.84190538, 0.8484158 , 0.84646267, 0.85742188,\n",
       "        0.8421224 , 0.83289931, 0.84505208, 0.81814236, 0.85394965,\n",
       "        0.85514323, 0.86957465, 0.83734809, 0.85894097, 0.85221354,\n",
       "        0.85134549, 0.85742187, 0.82530382, 0.84863281, 0.86903212,\n",
       "        0.85373264, 0.8421224 , 0.85644531, 0.83789062, 0.85763889,\n",
       "        0.85264757, 0.84385851, 0.85828993, 0.83561198, 0.84798177,\n",
       "        0.83886719, 0.86317274, 0.85112847, 0.83832465, 0.84928385,\n",
       "        0.85373264, 0.84624566, 0.86458333, 0.87141927, 0.86197917]),\n",
       " array([0.52170139, 0.53049045, 0.60373264, 0.6624349 , 0.67393663,\n",
       "        0.6140408 , 0.59928385, 0.64550781, 0.63845486, 0.67046441,\n",
       "        0.65277778, 0.62022569, 0.69270833, 0.6187066 , 0.66384549,\n",
       "        0.65321181, 0.64312066, 0.6374783 , 0.67317708, 0.68815104,\n",
       "        0.64344618, 0.70323351, 0.6577691 , 0.65017361, 0.64149306,\n",
       "        0.6422526 , 0.69791667, 0.71766493, 0.68098958, 0.70258247,\n",
       "        0.71657986, 0.67936198, 0.66015625, 0.69585503, 0.73730469,\n",
       "        0.69357639, 0.75358073, 0.72482639, 0.71158854, 0.71961806,\n",
       "        0.77115885, 0.78298611, 0.75043403, 0.77929688, 0.7609592 ,\n",
       "        0.77039931, 0.7875434 , 0.80479601, 0.78797743, 0.77745226,\n",
       "        0.78559028, 0.79448785, 0.78352865, 0.79513889, 0.78092448,\n",
       "        0.77018229, 0.78819444, 0.81586372, 0.7983941 , 0.81651476,\n",
       "        0.79709201, 0.80457899, 0.7578125 , 0.80891927, 0.79991319,\n",
       "        0.79774306, 0.79264323, 0.80175781, 0.79720052, 0.82769097,\n",
       "        0.81347656, 0.79633247, 0.81293403, 0.80609809, 0.81434462,\n",
       "        0.78895399, 0.79947917, 0.80197483, 0.81488715, 0.80978733,\n",
       "        0.79665799, 0.8125    , 0.80381944, 0.8297526 , 0.81477865,\n",
       "        0.83496094, 0.81445313, 0.81662326, 0.82899306, 0.84147135,\n",
       "        0.81174045, 0.83289931, 0.80197483, 0.85503472, 0.82020399,\n",
       "        0.84678819, 0.84559462, 0.83485243, 0.8530816 , 0.83995226,\n",
       "        0.84136285, 0.82432726, 0.85861545, 0.84277344, 0.83582899,\n",
       "        0.84125434, 0.85123698, 0.83680556, 0.8530816 , 0.86208767,\n",
       "        0.85839844, 0.84461806, 0.85796441, 0.8530816 , 0.85015191,\n",
       "        0.86284722, 0.84049479, 0.86859809, 0.85579427, 0.85970052,\n",
       "        0.85112847, 0.8578559 , 0.8625217 , 0.84375   , 0.84071181,\n",
       "        0.85264757, 0.85188802, 0.86968316, 0.87413194, 0.87196181,\n",
       "        0.86382378, 0.87402344, 0.8843316 , 0.8484158 , 0.84635417,\n",
       "        0.87076823, 0.85861545, 0.86078559, 0.8703342 , 0.8530816 ,\n",
       "        0.87250434, 0.87358941, 0.8796658 , 0.85861545, 0.86588542,\n",
       "        0.87803819, 0.86046007, 0.87478299, 0.87456597, 0.8734809 ,\n",
       "        0.86317274, 0.88172743, 0.86599392, 0.87858073, 0.85970052,\n",
       "        0.87044271, 0.89518229, 0.89431424, 0.87076823, 0.87554253,\n",
       "        0.87044271, 0.8671875 , 0.87879774, 0.88519965, 0.89767795,\n",
       "        0.89236111, 0.87565104, 0.86664497, 0.88617622, 0.875     ,\n",
       "        0.87803819, 0.86816406, 0.87565104, 0.87380642, 0.88064236,\n",
       "        0.87510851, 0.88682726, 0.88368056, 0.87749566, 0.85611979,\n",
       "        0.8640408 , 0.85828993, 0.87695312, 0.87782118, 0.88009983,\n",
       "        0.88519965, 0.85894097, 0.8671875 , 0.85872396, 0.87087674,\n",
       "        0.87955729, 0.86794705, 0.89431424, 0.87022569, 0.87478299,\n",
       "        0.86545139, 0.88085938, 0.87272135, 0.87901476, 0.8734809 ]),\n",
       " array([0.56852214, 0.55609809, 0.6125217 , 0.60763889, 0.55447049,\n",
       "        0.60362413, 0.55805122, 0.60948351, 0.62282986, 0.69585503,\n",
       "        0.63921441, 0.71875   , 0.71180556, 0.70616319, 0.77712674,\n",
       "        0.76974826, 0.74772135, 0.81944444, 0.76551649, 0.74490017,\n",
       "        0.78363715, 0.80609809, 0.75694444, 0.76182726, 0.75651042,\n",
       "        0.76692708, 0.76920573, 0.83789063, 0.73773872, 0.78797743,\n",
       "        0.75596788, 0.80685764, 0.78114149, 0.80403646, 0.72667101,\n",
       "        0.76009115, 0.74403212, 0.77951389, 0.80056424, 0.76215278,\n",
       "        0.71603733, 0.77246094, 0.77365451, 0.74978299, 0.76085069,\n",
       "        0.7515191 , 0.83572049, 0.82476128, 0.80132378, 0.78179253,\n",
       "        0.77115885, 0.81054688, 0.78233507, 0.77484809, 0.82595486,\n",
       "        0.79459635, 0.81477865, 0.80685764, 0.78797743, 0.78917101,\n",
       "        0.79893663, 0.81803385, 0.82703993, 0.80023872, 0.82725694,\n",
       "        0.82356771, 0.83962674, 0.81922743, 0.80110677, 0.88758681,\n",
       "        0.86946615, 0.83181424, 0.85080295, 0.81228299, 0.79654948,\n",
       "        0.83832465, 0.81032986, 0.84906684, 0.84592014, 0.82888455,\n",
       "        0.83962674, 0.84407552, 0.85872396, 0.85644531, 0.85047743,\n",
       "        0.86588542, 0.86197917, 0.85633681, 0.84407552, 0.84928385,\n",
       "        0.83821615, 0.87022569, 0.86165365, 0.82823351, 0.83561198,\n",
       "        0.8484158 , 0.87391493, 0.84288194, 0.81781684, 0.83072917,\n",
       "        0.85568576, 0.84971788, 0.85405816, 0.82855903, 0.83181424,\n",
       "        0.83517795, 0.87836372, 0.84320747, 0.84494358, 0.86783854,\n",
       "        0.86512587, 0.83702257, 0.86675347, 0.86512587, 0.84385851,\n",
       "        0.88053385, 0.86056858, 0.83648003, 0.83007813, 0.83669705,\n",
       "        0.86979167, 0.85904948, 0.83962674, 0.8843316 , 0.87358941,\n",
       "        0.85872396, 0.87174479, 0.88986545, 0.8327908 , 0.83973524,\n",
       "        0.86338976, 0.85687934, 0.84299045, 0.87380642, 0.85828993,\n",
       "        0.86002604, 0.84027778, 0.86773003, 0.88357205, 0.8374566 ,\n",
       "        0.83832465, 0.86273872, 0.8140191 , 0.85883247, 0.88009983,\n",
       "        0.86284722, 0.88140191, 0.8296441 , 0.87532552, 0.86219618,\n",
       "        0.87109375, 0.83767361, 0.85015191, 0.86783854, 0.88324653,\n",
       "        0.84320747, 0.86121962, 0.88704427, 0.89333767, 0.88107639,\n",
       "        0.85568576, 0.88454861, 0.8562283 , 0.89822049, 0.84700521,\n",
       "        0.82725694, 0.85883247, 0.87044271, 0.87098524, 0.87879774,\n",
       "        0.83930122, 0.84331597, 0.83789063, 0.86968316, 0.84429253,\n",
       "        0.89409722, 0.84016927, 0.85991753, 0.89203559, 0.85557726,\n",
       "        0.84288194, 0.84667969, 0.8608941 , 0.85818142, 0.84342448,\n",
       "        0.8484158 , 0.85839844, 0.82486979, 0.86783854, 0.86219618,\n",
       "        0.86664497, 0.86121962, 0.85449219, 0.8671875 , 0.85590278,\n",
       "        0.84038628, 0.85590278, 0.84461806, 0.8733724 , 0.86447483]),\n",
       " array([0.58951823, 0.62288411, 0.65733507, 0.62717014, 0.66438802,\n",
       "        0.65190972, 0.68348524, 0.7265625 , 0.68012153, 0.67366536,\n",
       "        0.70605469, 0.70171441, 0.68842231, 0.7016059 , 0.67165799,\n",
       "        0.69189453, 0.68066406, 0.68467882, 0.67778863, 0.67306858,\n",
       "        0.70507812, 0.70345052, 0.70507812, 0.73524306, 0.7014974 ,\n",
       "        0.73361545, 0.68934462, 0.75358073, 0.71137153, 0.78602431,\n",
       "        0.74110243, 0.73958333, 0.7875434 , 0.78027344, 0.76041667,\n",
       "        0.75716146, 0.79014757, 0.77170139, 0.80121528, 0.80414497,\n",
       "        0.79134115, 0.8030599 , 0.78700087, 0.8062066 , 0.7765842 ,\n",
       "        0.76117622, 0.80338542, 0.78515625, 0.83203125, 0.78569878,\n",
       "        0.80837674, 0.81022135, 0.83539497, 0.81901042, 0.77734375,\n",
       "        0.81521267, 0.7843967 , 0.84646267, 0.81434462, 0.79014757,\n",
       "        0.83289931, 0.82725694, 0.7985026 , 0.83170573, 0.80946181,\n",
       "        0.82628038, 0.8171658 , 0.82378472, 0.82378472, 0.80457899,\n",
       "        0.83637153, 0.84776476, 0.82584635, 0.82280816, 0.83930122,\n",
       "        0.83094618, 0.83713108, 0.83300781, 0.83409288, 0.82834201,\n",
       "        0.85243056, 0.85915799, 0.87608507, 0.81976997, 0.86317274,\n",
       "        0.81738281, 0.8249783 , 0.86111111, 0.83224826, 0.83648003,\n",
       "        0.86783854, 0.86078559, 0.84418403, 0.85557726, 0.85286458,\n",
       "        0.8562283 , 0.88400608, 0.84006076, 0.85557726, 0.87120226,\n",
       "        0.84895833, 0.84950087, 0.8343099 , 0.89192708, 0.86002604,\n",
       "        0.88151042, 0.87196181, 0.86273872, 0.86599392, 0.86480035,\n",
       "        0.83387587, 0.86425781, 0.87489149, 0.84743924, 0.84581163,\n",
       "        0.84461806, 0.87228733, 0.87022569, 0.86935764, 0.86338976,\n",
       "        0.8656684 , 0.87554253, 0.86903212, 0.85687934, 0.87586806,\n",
       "        0.87554253, 0.87000868, 0.87445747, 0.87597656, 0.85850694,\n",
       "        0.88422309, 0.85232205, 0.86968316, 0.8718533 , 0.87315538,\n",
       "        0.88671875, 0.88704427, 0.84798177, 0.88346354, 0.8781467 ,\n",
       "        0.88736979, 0.88378906, 0.88270399, 0.88953993, 0.88888889,\n",
       "        0.87055122, 0.88357205, 0.90180122, 0.88769531, 0.87293837,\n",
       "        0.87304688, 0.88270399, 0.86686198, 0.87326389, 0.85177951,\n",
       "        0.86480035, 0.87109375, 0.85536024, 0.87163628, 0.88031684,\n",
       "        0.87912326, 0.89930556, 0.88465712, 0.88042535, 0.89029948,\n",
       "        0.8889974 , 0.87293837, 0.85438368, 0.87239583, 0.86317274,\n",
       "        0.88400608, 0.8687066 , 0.87098524, 0.875     , 0.87402344,\n",
       "        0.8968099 , 0.89355469, 0.88693576, 0.87955729, 0.89019097,\n",
       "        0.90440538, 0.87934028, 0.90147569, 0.8734809 , 0.87456597,\n",
       "        0.87076823, 0.88096788, 0.88085937, 0.89702691, 0.88552517,\n",
       "        0.87847222, 0.87695312, 0.87727865, 0.88205295, 0.84798177,\n",
       "        0.89192708, 0.89301215, 0.87413194, 0.87109375, 0.87478299])]"
      ]
     },
     "execution_count": 58,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "validation_auroc_readings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "names.append('scr2')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.15rc1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
